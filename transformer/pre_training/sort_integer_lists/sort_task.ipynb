{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the parent directory (i.e. project root)\n",
    "project_root = Path().resolve().parent.parent \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformer.pre_training.sort_integer_lists.dataset import RandomIntegerDataset\n",
    "\n",
    "from src.embedding import CustomEmbedding\n",
    "from src.transformer import EncoderDecoderTransformer\n",
    "from src.utils import padding_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3593e+00, -1.8738e+00, -1.4160e+00,  1.8563e-02, -2.2517e-01,\n",
      "         -1.0900e+00,  5.5094e-01, -2.6020e-01, -1.8901e+00,  2.8605e+00,\n",
      "          1.5376e+00,  7.5922e-02, -2.2348e+00, -1.5930e+00,  5.2896e-01,\n",
      "         -7.8548e-01, -9.2142e-01, -5.8698e-01, -1.5417e+00,  2.2931e+00,\n",
      "          1.7263e+00, -1.1216e-01, -1.2678e+00, -9.0122e-01,  2.2178e+00,\n",
      "          8.6897e-01,  5.4969e-01,  7.9022e-01,  7.3506e-01, -1.9632e+00,\n",
      "         -3.4712e-01, -1.3287e-01, -2.2265e+00,  3.7048e-01,  4.8923e-01,\n",
      "         -7.2138e-01,  1.0277e+00, -5.5461e-01,  2.5897e-01,  2.2498e-01,\n",
      "         -1.0654e+00,  1.3920e+00, -5.3598e-01,  8.4914e-01,  1.1571e-01,\n",
      "          3.2136e-01,  1.4248e+00, -2.9901e-01, -9.3666e-01,  5.2934e-01,\n",
      "          1.2361e+00, -1.1005e+00, -1.6959e+00, -1.7556e+00,  9.2442e-02,\n",
      "         -4.5121e-01,  2.2787e-01,  1.8150e-01, -3.1161e-03, -9.6059e-01,\n",
      "          3.4410e-01,  5.5738e-01,  6.0925e-01,  6.4350e-01],\n",
      "        [-3.3884e-01, -9.4837e-01,  1.2291e+00, -7.4243e-01, -8.6878e-01,\n",
      "         -1.0758e-01,  6.3583e-01, -5.9789e-01, -1.5383e+00,  1.4961e-01,\n",
      "          6.4613e-02, -1.2606e+00, -3.9365e-01, -3.1968e-01, -2.5299e+00,\n",
      "         -1.2625e-01, -6.4148e-01,  9.0826e-02, -8.9019e-02, -2.0352e+00,\n",
      "          1.3476e+00, -2.2743e-01, -9.9493e-04,  1.1917e-01,  2.8198e-01,\n",
      "         -5.2553e-01, -3.3471e-01,  1.0383e+00, -3.6464e-01, -1.0239e+00,\n",
      "         -1.8065e-01, -1.5757e-01,  1.5081e-01,  7.7597e-01,  8.7118e-01,\n",
      "          3.7961e-01,  1.6996e+00, -8.4507e-01, -8.8403e-01, -1.7184e-01,\n",
      "          1.4839e+00, -2.6718e-01, -5.6670e-01, -9.7724e-01,  7.9870e-01,\n",
      "          1.6599e-01, -1.0711e+00,  1.4856e+00, -3.7918e-01, -4.1116e-01,\n",
      "         -6.4783e-01, -4.4695e-01,  1.6347e-01, -7.0518e-01, -6.2826e-01,\n",
      "         -2.5720e-01,  1.1801e+00, -2.2683e-01,  3.2899e-01,  8.3917e-01,\n",
      "          4.5049e-01, -4.3138e-02, -1.5539e-01,  8.1809e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_real_tokens = 10\n",
    "PAD_TOKEN_IDX = n_real_tokens\n",
    "SOS_TOKEN_IDX = n_real_tokens + 1\n",
    "EOS_TOKEN_IDX = n_real_tokens + 2\n",
    "vocab_size = n_real_tokens + 3\n",
    "D_MODEL = 64\n",
    "\n",
    "embeddings = CustomEmbedding(vocab_size, d_model = D_MODEL) # 3 = PAD, SOS, EOS\n",
    "\n",
    "indices = torch.tensor([1,9])\n",
    "\n",
    "# print(embeddings.embeddings.weight)\n",
    "print(embeddings.embeddings(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT_WINDOW = 50\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MIN_SEQ_LEN = 2\n",
    "MAX_SEQ_LEN = min(20, MAX_CONTEXT_WINDOW)\n",
    "\n",
    "NUM_TRAINING_SEQUENCES = 10000\n",
    "NUM_VALIDATION_SEQUENCES = 1000\n",
    "\n",
    "VOCAB = [i for i in range(n_real_tokens)] # does not include SOS, EOS, PAD\n",
    "\n",
    "VOCAB_MAP = dict()\n",
    "\n",
    "for i, token in enumerate(VOCAB):\n",
    "    VOCAB_MAP[i] = token\n",
    "VOCAB_MAP[len(VOCAB_MAP)] = '<PAD>'\n",
    "VOCAB_MAP[len(VOCAB_MAP) + 1] = '<SOS>'\n",
    "VOCAB_MAP[len(VOCAB_MAP) + 2] = '<EOS>'\n",
    "\n",
    "train_rand_ds = RandomIntegerDataset(MIN_SEQ_LEN, MAX_SEQ_LEN, NUM_TRAINING_SEQUENCES, VOCAB)\n",
    "train_dataloader = DataLoader(train_rand_ds, batch_size = BATCH_SIZE, shuffle = True, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))\n",
    "\n",
    "val_rand_ds = RandomIntegerDataset(MIN_SEQ_LEN, MAX_SEQ_LEN, NUM_VALIDATION_SEQUENCES, VOCAB)\n",
    "val_dataloader = DataLoader(val_rand_ds, batch_size = BATCH_SIZE, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9,  7,  9,  ..., 10, 10, 10],\n",
      "        [ 9,  2,  8,  ..., 10, 10, 10],\n",
      "        [ 8,  7,  6,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 5,  2,  9,  ...,  5, 10, 10],\n",
      "        [ 7,  1,  3,  ..., 10, 10, 10],\n",
      "        [ 6,  8,  8,  ..., 10, 10, 10]])\n",
      "tensor([[11,  0,  0,  ..., 10, 10, 10],\n",
      "        [11,  0,  1,  ..., 10, 10, 10],\n",
      "        [11,  0,  2,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [11,  0,  0,  ...,  9, 10, 10],\n",
      "        [11,  1,  1,  ..., 10, 10, 10],\n",
      "        [11,  1,  3,  ..., 10, 10, 10]])\n",
      "tensor([[ 0,  0,  5,  ..., 10, 10, 10],\n",
      "        [ 0,  1,  1,  ..., 10, 10, 10],\n",
      "        [ 0,  2,  2,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 0,  0,  2,  ..., 12, 10, 10],\n",
      "        [ 1,  1,  2,  ..., 10, 10, 10],\n",
      "        [ 1,  3,  5,  ..., 10, 10, 10]])\n"
     ]
    }
   ],
   "source": [
    "input, label = next(iter(train_dataloader))\n",
    "print(input[0])\n",
    "print(input[1])\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/opt/miniconda3/envs/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_IDX, reduction = 'sum')\n",
    "\n",
    "model = EncoderDecoderTransformer(\n",
    "                    embeddings = embeddings, \n",
    "                    vocab_size = vocab_size, \n",
    "                    d_model = D_MODEL, \n",
    "                    num_attention_heads = 4, \n",
    "                    num_encoder_layers = 2, \n",
    "                    num_decoder_layers = 2, \n",
    "                    dim_feedforward = 32, \n",
    "                    dropout = 0.0,\n",
    "                    max_context_window = MAX_CONTEXT_WINDOW,\n",
    "                    use_pre_lnorm = True)\n",
    "\n",
    "optim = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum = 0.9, weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(source: torch.Tensor, model: nn.Module) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Designed to do autoregressive inference in an Encoder-Decoder transformer.\n",
    "\n",
    "    This greedy decoder always predicts the vocabulary token corresponding to the highest logit.\n",
    "\n",
    "    Takes the source sequence and the Encoder-Decoder to produce a predicted\n",
    "    sequence starting from <SOS>.\n",
    "\n",
    "    Note: This function *can* handle batches of sequences.\n",
    "\n",
    "    Args:\n",
    "        source - The source sequence to be passed to the Transformer's encoder block.\n",
    "        model - The Encoder-Decoder transformer over which to greedy decode\n",
    "\n",
    "    Returns:\n",
    "        target - The batch of predicted sequences corresponding to the input sources.\n",
    "        target_logits - (# batch elements =) batch_size (# rows =) seq_len (# cols =) vocab-dimensional vectors, each of which\n",
    "        corresponds to the set of logits on a particular inference step within a given sequence.\n",
    "    \"\"\"\n",
    "    batch_size = source.size(dim = 0)\n",
    "\n",
    "    encoder_output, source_pad_mask = model.encode(source)\n",
    "\n",
    "    # target will contain num_batch sequences of indices that are the predicted next-words for each batch element\n",
    "    target = torch.full((batch_size, 1), SOS_TOKEN_IDX) # target.shape: [batch_size, num_loops_complete - 1]\n",
    "    target_logits = torch.zeros((batch_size, 1, vocab_size))\n",
    "\n",
    "    finished = torch.full((batch_size, ), False)\n",
    "\n",
    "    while not finished.all() and target.size(dim = 1) <= MAX_CONTEXT_WINDOW:\n",
    "\n",
    "        decoder_output, _ = model.decode(target, encoder_output, source_pad_mask)\n",
    "        pred_logits = model.project_into_vocab(decoder_output) # pred_logits.shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        last_row_pred_logits = pred_logits[:, -1, :] # last_row_pred_logits.shape == [batch_size, vocab_size]\n",
    "\n",
    "        # Track next-word logits for loss_fn later.\n",
    "        target_logits = torch.concat((target_logits, last_row_pred_logits.unsqueeze(1)), dim = 1)\n",
    "\n",
    "        predictions = torch.argmax(last_row_pred_logits, dim = -1) # predictions.shape: [batch_size]\n",
    "\n",
    "        # For any finished sequences (i.e. previous EOS-producers), force their prediction from this round to be a pad.\n",
    "        predictions[finished] = PAD_TOKEN_IDX\n",
    "\n",
    "        # Mark any additional sequences that just produced an EOS as finished.\n",
    "        finished |= predictions == EOS_TOKEN_IDX\n",
    "\n",
    "        target = torch.concat((target, predictions.reshape(-1, 1)), dim = 1) # target.shape: [batch_size, num_loops_complete]\n",
    "\n",
    "    return target, target_logits[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_epoch(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, optimizer: torch.optim.Optimizer, calculate_sequence_accuracy: bool = False, calculate_token_accuracy: bool = False):\n",
    "    \"\"\"\n",
    "    Runs one training epoch (processing the entire training dataset once).\n",
    "    \n",
    "    Uses Teacher Forcing to train token-to-token mapping quality without cascading errors and for parallelization.\n",
    "\n",
    "    Args:\n",
    "        dataloader - The dataloader to process the dataset in BATCH_SIZE batches\n",
    "        model - The Encoder-Decoder that is being trained\n",
    "        loss_fn - The loss function to calculate the model's correctness\n",
    "        optimizer - The optimizer to improve the model's weights\n",
    "        calculate_sequence_accuracy - A flag to mark whether sequence-level correctness should be tracked\n",
    "        calculate_token_accuracy - A flag to mark whether token-level correctness should be tracked\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    num_sequences = len(dataloader.dataset)\n",
    "    num_tokens = 0\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    total_correct_sequences = 0\n",
    "    total_correct_tokens = 0\n",
    "\n",
    "    for (source, target), label in tqdm(dataloader):\n",
    "\n",
    "        # FORWARD\n",
    "        pred_logits = model(source, target)\n",
    "\n",
    "        # pred_logits.shape: [batch_size, seq_len, vocab_size]\n",
    "        # label.shape: [batch_size, seq_len]\n",
    "\n",
    "        # CrossEntropyLoss (loss_fn) only takes 2D predictions (n_batch * seq_len, vocab_size) and 1D labels (n_batch * seq_len)\n",
    "        batch_loss = loss_fn(pred_logits.view(-1, pred_logits.size(-1)), label.view(-1))\n",
    "\n",
    "        # LOG\n",
    "        with torch.no_grad():\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "            predictions = torch.argmax(pred_logits, dim = -1) # predictions.shape: [batch_size, seq_len]\n",
    "            match_matrix = torch.eq(predictions, label)\n",
    "\n",
    "            if calculate_sequence_accuracy:\n",
    "                num_correct_sequences = torch.all(match_matrix, dim = 1).sum()\n",
    "                total_correct_sequences += num_correct_sequences.item()\n",
    "\n",
    "            if calculate_token_accuracy:\n",
    "                num_correct_tokens = match_matrix.sum()      \n",
    "                total_correct_tokens += num_correct_tokens.item()\n",
    "\n",
    "                num_tokens += torch.numel(label)\n",
    "\n",
    "        # BACKWARD\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # OPTIMIZE\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    average_epoch_loss = epoch_loss / num_sequences\n",
    "    average_epoch_sequence_accuracy = total_correct_sequences / num_sequences if calculate_sequence_accuracy else None\n",
    "    average_epoch_token_accuracy = total_correct_tokens / num_tokens if calculate_token_accuracy else None\n",
    "\n",
    "    return average_epoch_loss, average_epoch_sequence_accuracy, average_epoch_token_accuracy\n",
    "\n",
    "def run_gold_validation_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, calculate_sequence_accuracy: bool = False, calculate_token_accuracy: bool = False):\n",
    "    \"\"\"\n",
    "    Runs one validation epoch (processing the entire validation dataset once). \n",
    "\n",
    "    Uses Teacher Forcing (i.e. \"gold\") to evaluate token-to-token mapping quality and for parallelization.\n",
    "\n",
    "    Args:\n",
    "        dataloader - The dataloader to process the dataset in BATCH_SIZE batches\n",
    "        model - The Encoder-Decoder that is being trained\n",
    "        loss_fn - The loss function to calculate the model's correctness\n",
    "        calculate_sequence_accuracy - A flag to mark whether sequence-level correctness should be tracked\n",
    "        calculate_token_accuracy - A flag to mark whether token-level correctness should be tracked\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    num_sequences = len(dataloader.dataset)\n",
    "    num_tokens = 0\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    total_correct_sequences = 0\n",
    "    total_correct_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (source, target), label in tqdm(dataloader):\n",
    "            \n",
    "            # FORWARD\n",
    "            pred_logits = model(source, target)\n",
    "            batch_loss = loss_fn(pred_logits.view(-1, pred_logits.size(-1)), label.view(-1))\n",
    "\n",
    "            # LOG\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "            predictions = torch.argmax(pred_logits, dim = -1) # predictions.shape: [batch_size, seq_len]\n",
    "            match_matrix = torch.eq(predictions, label)\n",
    "\n",
    "            if calculate_sequence_accuracy:\n",
    "                num_correct_sequences = torch.all(match_matrix, dim = 1).sum()\n",
    "                total_correct_sequences += num_correct_sequences.item()\n",
    "\n",
    "            if calculate_token_accuracy:\n",
    "                num_correct_tokens = match_matrix.sum()      \n",
    "                total_correct_tokens += num_correct_tokens.item()\n",
    "\n",
    "                num_tokens += torch.numel(label)\n",
    "\n",
    "    average_epoch_loss = epoch_loss / num_sequences\n",
    "    average_epoch_sequence_accuracy = total_correct_sequences / num_sequences if calculate_sequence_accuracy else None\n",
    "    average_epoch_token_accuracy = total_correct_tokens / num_tokens if calculate_token_accuracy else None\n",
    "\n",
    "    return average_epoch_loss, average_epoch_sequence_accuracy, average_epoch_token_accuracy\n",
    "\n",
    "def run_autoregressive_validation_loop(dataloader: DataLoader, model: nn.Module):\n",
    "    \"\"\"\n",
    "    Runs one autoregressive validation epoch (processing the entire validation dataset once). \n",
    "\n",
    "    Args:\n",
    "        dataloader - The dataloader to process the dataset in BATCH_SIZE batches\n",
    "        model - The Encoder-Decoder that is being trained\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    correct_sequences = 0\n",
    "    incorrect_sequences = 0\n",
    "    total_sequences = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (source, _), label in tqdm(dataloader):\n",
    "\n",
    "            # FORWARD\n",
    "            pred_indices, pred_logits = greedy_decode(source, model)\n",
    "\n",
    "            np_source_indices = source.numpy().copy()\n",
    "            np_pred_target_indices = pred_indices.numpy().copy()\n",
    "\n",
    "            token_values = np.array(list(VOCAB_MAP.values()))\n",
    "            predicted_source_tokens = token_values[np_source_indices]\n",
    "            predicted_target_tokens = token_values[np_pred_target_indices]\n",
    "\n",
    "            for s, t in zip(predicted_source_tokens, predicted_target_tokens):\n",
    "                source_end_index = np.argmax(s == '<PAD>')  if '<PAD>' in s else len(s)\n",
    "                target_end_index = np.argmax(t == '<EOS>')\n",
    "                if np.array_equal(np.sort(s[:source_end_index]), t[1:target_end_index]):\n",
    "                    correct_sequences += 1\n",
    "                else:\n",
    "                    incorrect_sequences += 1\n",
    "                    print(f'Incorrect Sequence {incorrect_sequences}:')\n",
    "                    print(np.sort(s[:source_end_index]))\n",
    "                    print(t[1:target_end_index])\n",
    "                    print(f'{'Source:':<20} {s}\\n{'Predicted Target:':<20} {t}', end = '\\n\\n')\n",
    "\n",
    "            total_sequences += predicted_target_tokens.shape[0]\n",
    "\n",
    "    return correct_sequences / total_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:09<00:00, 17.38it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 47.70it/s]\n",
      "100%|██████████| 157/157 [00:08<00:00, 19.19it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 57.21it/s]\n",
      "100%|██████████| 157/157 [00:08<00:00, 19.59it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 57.31it/s]\n",
      "100%|██████████| 157/157 [00:08<00:00, 19.19it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 57.17it/s]\n",
      "100%|██████████| 157/157 [00:08<00:00, 18.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 53.58it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 17.12it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 53.71it/s]\n",
      "100%|██████████| 157/157 [00:08<00:00, 18.89it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 55.43it/s]\n",
      "100%|██████████| 157/157 [00:08<00:00, 19.05it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 55.37it/s]\n",
      "100%|██████████| 157/157 [00:09<00:00, 16.94it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 43.63it/s]\n",
      "100%|██████████| 157/157 [00:08<00:00, 19.12it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 51.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.349227062988281, 1.903295444869995, 0.8003001699447632, 0.8503367011547088, 0.6524146330356598, 0.24151200650930404, 0.2364796200990677, 0.6866460898399352, 0.2824481308877468, 0.033657983401417735]\n",
      "[0.0015, 0.0135, 0.027, 0.026, 0.0351, 0.0474, 0.0465, 0.0388, 0.0494, 0.0563]\n",
      "[0.4545991915185722, 0.541941856913797, 0.5598696248856359, 0.5582389613360788, 0.5629674145299145, 0.5699728779891512, 0.5696750248072666, 0.5632823830241966, 0.5698223156285823, 0.57234180596901]\n",
      "\n",
      "[5.321473709106446, 3.8494616088867186, 1.3720016479492188, 0.49469689750671386, 0.23472629165649414, 1.3920778617858887, 4.330578262329102, 4.869108871459961, 0.0588050457239151, 0.016656220108270645]\n",
      "[0.0, 0.007, 0.024, 0.039, 0.036, 0.023, 0.001, 0.005, 0.048, 0.05]\n",
      "[0.4797193387158785, 0.5456074586697424, 0.5594482891195693, 0.576316801230296, 0.5804978854286813, 0.5600249903883122, 0.5266243752402922, 0.5505094194540562, 0.5827566320645905, 0.5831891580161477]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "training_losses = list()\n",
    "training_sequence_accuracies = list()\n",
    "training_token_accuracies = list()\n",
    "\n",
    "gold_validation_losses = list()\n",
    "gold_validation_sequence_accuracies = list()\n",
    "gold_validation_token_accuracies = list()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # print(f'Running epoch {i+1}...')\n",
    "\n",
    "    training_loss, training_sequence_accuracy, training_token_accuracy = run_train_epoch(train_dataloader, model, loss_fn, optim, calculate_sequence_accuracy = True, calculate_token_accuracy = True)\n",
    "\n",
    "    training_losses.append(training_loss)\n",
    "    training_sequence_accuracies.append(training_sequence_accuracy)\n",
    "    training_token_accuracies.append(training_token_accuracy)\n",
    "\n",
    "    gold_val_loss, gold_val_sequence_accuracy, gold_val_token_accuracy = run_gold_validation_loop(val_dataloader, model, loss_fn, calculate_sequence_accuracy = True, calculate_token_accuracy = True)\n",
    "    \n",
    "    gold_validation_losses.append(gold_val_loss)\n",
    "    gold_validation_sequence_accuracies.append(gold_val_sequence_accuracy)\n",
    "    gold_validation_token_accuracies.append(gold_val_token_accuracy)\n",
    "\n",
    "print(training_losses)\n",
    "print(training_sequence_accuracies)\n",
    "print(training_token_accuracies)\n",
    "\n",
    "print()\n",
    "\n",
    "print(gold_validation_losses)\n",
    "print(gold_validation_sequence_accuracies)\n",
    "print(gold_validation_token_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:01<00:01,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sequence 1:\n",
      "['0' '0' '0' '8']\n",
      "['0' '0' '0' '0' '8' '8']\n",
      "Source:              ['0' '0' '0' '8' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "Predicted Target:    ['<SOS>' '0' '0' '0' '0' '8' '8' '<EOS>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [00:01<00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sequence 2:\n",
      "['2' '3' '3' '3' '3' '4']\n",
      "['2' '3' '3' '3' '3' '3' '4']\n",
      "Source:              ['2' '3' '3' '3' '4' '3' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "Predicted Target:    ['<SOS>' '2' '3' '3' '3' '3' '3' '4' '<EOS>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc = run_autoregressive_validation_loop(val_dataloader, model)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
