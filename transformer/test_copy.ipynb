{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "from embedding import CustomEmbedding\n",
    "from transformers import EncoderDecoderTransformer\n",
    "from utils import padding_collate_fn\n",
    "\n",
    "from generate_data import RandomIntegerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4959,  1.1532, -1.5517, -0.0264, -1.2842, -0.2948,  0.1843,  2.5940,\n",
      "          0.2433,  0.3829,  1.2278, -0.0028, -0.4759, -0.9599,  1.4245,  0.8282],\n",
      "        [-1.5257, -1.3818, -0.7827, -0.7115, -0.7646, -0.9675, -0.9097, -0.6993,\n",
      "          2.6623, -0.0931,  1.1417, -0.2799, -1.7919, -0.7490,  1.1308,  0.1573]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_real_tokens = 10\n",
    "PAD_TOKEN_IDX = n_real_tokens\n",
    "SOS_TOKEN_IDX = n_real_tokens + 1\n",
    "EOS_TOKEN_IDX = n_real_tokens + 2\n",
    "vocab_size = n_real_tokens + 3\n",
    "D_MODEL = 16\n",
    "\n",
    "embeddings = CustomEmbedding(vocab_size = vocab_size + 3, d_model = D_MODEL) # 3 = PAD, SOS, EOS\n",
    "\n",
    "indices = torch.tensor([1,9])\n",
    "\n",
    "# print(embeddings.embeddings.weight)\n",
    "print(embeddings.embeddings(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT_WINDOW = 50\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MIN_SEQ_LEN = 2\n",
    "MAX_SEQ_LEN = min(10, MAX_CONTEXT_WINDOW)\n",
    "\n",
    "rand_ds = RandomIntegerDataset(2, 10, 10000, [i for i in range(10)])\n",
    "dataloader = DataLoader(rand_ds, batch_size = BATCH_SIZE, shuffle = True, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8,  5,  0,  2,  6, 10, 10, 10, 10, 10],\n",
      "        [ 6,  6,  1,  7, 10, 10, 10, 10, 10, 10],\n",
      "        [ 1,  5,  1,  1, 10, 10, 10, 10, 10, 10],\n",
      "        [ 4,  4,  8,  3, 10, 10, 10, 10, 10, 10],\n",
      "        [ 2,  1,  8,  2,  4,  0,  0,  3,  1, 10],\n",
      "        [ 5,  7,  6,  0,  0, 10, 10, 10, 10, 10],\n",
      "        [ 6,  8,  0,  1,  6, 10, 10, 10, 10, 10],\n",
      "        [ 5,  4,  4,  3,  3,  1,  6, 10, 10, 10],\n",
      "        [ 0,  2,  8,  0,  4,  3,  9,  1,  1, 10],\n",
      "        [ 9,  2,  7,  5,  9,  0, 10, 10, 10, 10],\n",
      "        [ 2,  5,  3,  4, 10, 10, 10, 10, 10, 10],\n",
      "        [ 9,  9,  7,  5,  0,  4, 10, 10, 10, 10],\n",
      "        [ 7,  2,  3,  7,  2,  9,  9,  4,  7, 10],\n",
      "        [ 8,  8, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 5,  0,  9,  5,  4,  6,  0,  5,  0, 10],\n",
      "        [ 8,  7,  0,  7, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  3,  1,  0,  9,  4,  4,  6,  7,  0],\n",
      "        [ 0,  9,  3, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 2,  2,  1,  7,  2,  1,  0,  0,  1, 10],\n",
      "        [ 2,  7,  7,  0,  2, 10, 10, 10, 10, 10],\n",
      "        [ 1,  5,  1,  9,  6,  6,  4,  7,  1, 10],\n",
      "        [ 1,  4,  6, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  3, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 9,  6,  8,  9,  6,  6,  9,  4, 10, 10],\n",
      "        [ 4,  8,  3,  5,  0,  8,  5, 10, 10, 10],\n",
      "        [ 2,  2,  1,  5,  8,  8,  1,  4,  8, 10],\n",
      "        [ 2,  6,  2,  3,  3,  6,  9,  2, 10, 10],\n",
      "        [ 9,  0,  9,  1,  2,  1,  2,  7, 10, 10],\n",
      "        [ 8,  3,  7,  4,  1,  9, 10, 10, 10, 10],\n",
      "        [ 4,  5,  0, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 7,  4,  9,  1,  7, 10, 10, 10, 10, 10],\n",
      "        [ 6,  6,  7,  6,  7,  4, 10, 10, 10, 10]])\n",
      "tensor([[11,  0,  2,  5,  6,  8, 10, 10, 10, 10, 10],\n",
      "        [11,  1,  6,  6,  7, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  1,  1,  1,  5, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  3,  4,  4,  8, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  0,  1,  1,  2,  2,  3,  4,  8, 10],\n",
      "        [11,  0,  0,  5,  6,  7, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  1,  6,  6,  8, 10, 10, 10, 10, 10],\n",
      "        [11,  1,  3,  3,  4,  4,  5,  6, 10, 10, 10],\n",
      "        [11,  0,  0,  1,  1,  2,  3,  4,  8,  9, 10],\n",
      "        [11,  0,  2,  5,  7,  9,  9, 10, 10, 10, 10],\n",
      "        [11,  2,  3,  4,  5, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  4,  5,  7,  9,  9, 10, 10, 10, 10],\n",
      "        [11,  2,  2,  3,  4,  7,  7,  7,  9,  9, 10],\n",
      "        [11,  8,  8, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  0,  0,  4,  5,  5,  5,  6,  9, 10],\n",
      "        [11,  0,  7,  7,  8, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  0,  0,  1,  3,  4,  4,  6,  7,  9],\n",
      "        [11,  0,  3,  9, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  0,  1,  1,  1,  2,  2,  2,  7, 10],\n",
      "        [11,  0,  2,  2,  7,  7, 10, 10, 10, 10, 10],\n",
      "        [11,  1,  1,  1,  4,  5,  6,  6,  7,  9, 10],\n",
      "        [11,  1,  4,  6, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  3, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  4,  6,  6,  6,  8,  9,  9,  9, 10, 10],\n",
      "        [11,  0,  3,  4,  5,  5,  8,  8, 10, 10, 10],\n",
      "        [11,  1,  1,  2,  2,  4,  5,  8,  8,  8, 10],\n",
      "        [11,  2,  2,  2,  3,  3,  6,  6,  9, 10, 10],\n",
      "        [11,  0,  1,  1,  2,  2,  7,  9,  9, 10, 10],\n",
      "        [11,  1,  3,  4,  7,  8,  9, 10, 10, 10, 10],\n",
      "        [11,  0,  4,  5, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  1,  4,  7,  7,  9, 10, 10, 10, 10, 10],\n",
      "        [11,  4,  6,  6,  6,  7,  7, 10, 10, 10, 10]])\n",
      "tensor([[ 0,  2,  5,  6,  8, 12, 10, 10, 10, 10, 10],\n",
      "        [ 1,  6,  6,  7, 12, 10, 10, 10, 10, 10, 10],\n",
      "        [ 1,  1,  1,  5, 12, 10, 10, 10, 10, 10, 10],\n",
      "        [ 3,  4,  4,  8, 12, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  0,  1,  1,  2,  2,  3,  4,  8, 12, 10],\n",
      "        [ 0,  0,  5,  6,  7, 12, 10, 10, 10, 10, 10],\n",
      "        [ 0,  1,  6,  6,  8, 12, 10, 10, 10, 10, 10],\n",
      "        [ 1,  3,  3,  4,  4,  5,  6, 12, 10, 10, 10],\n",
      "        [ 0,  0,  1,  1,  2,  3,  4,  8,  9, 12, 10],\n",
      "        [ 0,  2,  5,  7,  9,  9, 12, 10, 10, 10, 10],\n",
      "        [ 2,  3,  4,  5, 12, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  4,  5,  7,  9,  9, 12, 10, 10, 10, 10],\n",
      "        [ 2,  2,  3,  4,  7,  7,  7,  9,  9, 12, 10],\n",
      "        [ 8,  8, 12, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  0,  0,  4,  5,  5,  5,  6,  9, 12, 10],\n",
      "        [ 0,  7,  7,  8, 12, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  0,  0,  1,  3,  4,  4,  6,  7,  9, 12],\n",
      "        [ 0,  3,  9, 12, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  0,  1,  1,  1,  2,  2,  2,  7, 12, 10],\n",
      "        [ 0,  2,  2,  7,  7, 12, 10, 10, 10, 10, 10],\n",
      "        [ 1,  1,  1,  4,  5,  6,  6,  7,  9, 12, 10],\n",
      "        [ 1,  4,  6, 12, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  3, 12, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 4,  6,  6,  6,  8,  9,  9,  9, 12, 10, 10],\n",
      "        [ 0,  3,  4,  5,  5,  8,  8, 12, 10, 10, 10],\n",
      "        [ 1,  1,  2,  2,  4,  5,  8,  8,  8, 12, 10],\n",
      "        [ 2,  2,  2,  3,  3,  6,  6,  9, 12, 10, 10],\n",
      "        [ 0,  1,  1,  2,  2,  7,  9,  9, 12, 10, 10],\n",
      "        [ 1,  3,  4,  7,  8,  9, 12, 10, 10, 10, 10],\n",
      "        [ 0,  4,  5, 12, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 1,  4,  7,  7,  9, 12, 10, 10, 10, 10, 10],\n",
      "        [ 4,  6,  6,  6,  7,  7, 12, 10, 10, 10, 10]])\n"
     ]
    }
   ],
   "source": [
    "input, label = next(iter(dataloader))\n",
    "print(input[0])\n",
    "print(input[1])\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_IDX, reduction = 'sum')\n",
    "\n",
    "model = EncoderDecoderTransformer(\n",
    "                    embeddings = embeddings, \n",
    "                    vocab_size = vocab_size, \n",
    "                    d_model = D_MODEL, \n",
    "                    num_attention_heads = 4, \n",
    "                    num_encoder_layers = 1, \n",
    "                    num_decoder_layers = 1, \n",
    "                    dim_feedforward = 32, \n",
    "                    max_context_window = MAX_CONTEXT_WINDOW)\n",
    "\n",
    "optim = torch.optim.SGD(params = model.parameters(), lr = 1e-3, momentum = 0.9, weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_epoch(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, ((source, target), label) in tqdm(enumerate(dataloader)):\n",
    "\n",
    "        # FORWARD\n",
    "        pred_logits = model(source, target)\n",
    "        # CrossEntropyLoss (loss_fn) only takes 2D predictions (n_batch * seq_len, vocab_size) and 1D labels (n_batch * seq_len)\n",
    "        batch_loss = loss_fn(pred_logits.view(-1, vocab_size), label.view(-1))\n",
    "\n",
    "        # LOG\n",
    "        epoch_loss += batch_loss.item()\n",
    "\n",
    "        # BACKWARD\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # OPTIMIZE\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    average_epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "\n",
    "    return average_epoch_loss\n",
    "\n",
    "def run_validation_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "    \n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 130.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 132.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 136.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 138.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 140.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 141.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 140.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 138.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 141.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:02, 139.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.998334381866455, 3.1808130752563475, 2.8981128993988037, 2.335165265083313, 2.150253467941284, 2.341424671936035, 2.176064828491211, 2.0349822174072267, 1.8267414445877075, 2.13696817111969]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "training_losses = list()\n",
    "validation_losses = list()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(f'Running epoch {i+1}...')\n",
    "\n",
    "    training_loss = run_train_epoch(dataloader, model, loss_fn, optim)\n",
    "    training_losses.append(training_loss)\n",
    "\n",
    "    # validation_loss = run_validation_loop(dataloader, model, loss_fn)\n",
    "    # validation_losses.append(validation_loss)\n",
    "\n",
    "print(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
