{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f8197d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "48e3952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the parent directory (i.e. project root)\n",
    "project_root = Path().resolve().parent.parent \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenization.byte_pair_encoding.get_tokenizers import train_and_save_tokenizer_for, load_tokenizer_from\n",
    "\n",
    "from pre_training.text_summarization.dataset import TextSummarizationDataset\n",
    "\n",
    "from src.embedding import CustomEmbedding\n",
    "from src.transformer import EncoderDecoderTransformer\n",
    "from src.utils import list_padding_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "81921cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_DATA_PATH = '../../data/SAMSum/'\n",
    "\n",
    "BPE_IN_PATH = '../../data/SAMSum/train_summary_and_dialogue.txt'\n",
    "BPE_OUT_PATH = '../../tokenization/trained_tokenizers/SAMSum_BPE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0a22ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT_WINDOW = 100\n",
    "\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "73f11a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(DF_DATA_PATH + 'train_df.json', orient = 'records', lines = True)\n",
    "val_df = pd.read_json(DF_DATA_PATH + 'val_df.json', orient = 'records', lines = True)\n",
    "test_df = pd.read_json(DF_DATA_PATH + 'test_df.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "319eede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The pad token index is 2.\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = train_and_save_tokenizer_for(in_file_paths = [BPE_IN_PATH], out_file_dir_path = BPE_OUT_PATH, vocab_size = 4_000)\n",
    "pretrained_bpe_tokenizer = load_tokenizer_from(dir_path = BPE_OUT_PATH, model_max_length = 10000)\n",
    "\n",
    "PAD_TOKEN_IDX = pretrained_bpe_tokenizer.pad_token_id\n",
    "print(f'The pad token index is {PAD_TOKEN_IDX}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8a000cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a max_context_window of 100...\n",
      "The number of training samples went from 14732 to 5580\n",
      "The number of validation samples went from 818 to 325\n",
      "The number of test samples went from 819 to 308\n"
     ]
    }
   ],
   "source": [
    "FILTER_tokenized_train_sources = pretrained_bpe_tokenizer(\n",
    "    train_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_train_targets = pretrained_bpe_tokenizer(\n",
    "    train_df['summary'].tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_val_sources = pretrained_bpe_tokenizer(\n",
    "    val_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_val_targets = pretrained_bpe_tokenizer(\n",
    "    val_df['summary'].tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_test_sources = pretrained_bpe_tokenizer(\n",
    "    test_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_test_targets = pretrained_bpe_tokenizer(\n",
    "    test_df['summary'].tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "valid_src_train_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_train_sources.data['input_ids']])\n",
    "valid_src_val_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_val_sources.data['input_ids']])\n",
    "valid_src_test_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_test_sources.data['input_ids']])\n",
    "\n",
    "valid_tgt_train_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW - 1 for example in FILTER_tokenized_train_targets.data['input_ids']])\n",
    "valid_tgt_val_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW - 1 for example in FILTER_tokenized_val_targets.data['input_ids']])\n",
    "valid_tgt_test_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW - 1 for example in FILTER_tokenized_test_targets.data['input_ids']])\n",
    "\n",
    "valid_train_df = train_df.iloc[valid_src_train_indices & valid_tgt_train_indices]\n",
    "valid_val_df = val_df.iloc[valid_src_val_indices & valid_tgt_val_indices]\n",
    "valid_test_df = test_df.iloc[valid_src_test_indices & valid_tgt_test_indices]\n",
    "\n",
    "print(f'With a max_context_window of {MAX_CONTEXT_WINDOW}...')\n",
    "print(f'The number of training samples went from {train_df.shape[0]} to {valid_train_df.shape[0]}')\n",
    "print(f'The number of validation samples went from {val_df.shape[0]} to {valid_val_df.shape[0]}')\n",
    "print(f'The number of test samples went from {test_df.shape[0]} to {valid_test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "686aeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sources = pretrained_bpe_tokenizer(\n",
    "    valid_train_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "tokenized_train_targets = pretrained_bpe_tokenizer(\n",
    "    ('<SOS> ' + valid_train_df['summary']).tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "tokenized_train_labels = pretrained_bpe_tokenizer(\n",
    "    (valid_train_df['summary'] + ' <EOS>').tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "tokenized_val_sources = pretrained_bpe_tokenizer(\n",
    "    valid_val_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "tokenized_val_targets = pretrained_bpe_tokenizer(\n",
    "    ('<SOS> ' + valid_val_df['summary']).tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "tokenized_val_labels = pretrained_bpe_tokenizer(\n",
    "    (valid_val_df['summary'] + ' <EOS>').tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "tokenized_test_sources = pretrained_bpe_tokenizer(\n",
    "    valid_test_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "tokenized_test_targets = pretrained_bpe_tokenizer(\n",
    "    ('<SOS> ' + valid_test_df['summary']).tolist(),\n",
    "    add_special_tokens = False\n",
    ")\n",
    "\n",
    "tokenized_test_labels = pretrained_bpe_tokenizer(\n",
    "    (valid_test_df['summary'] + ' <EOS>').tolist(),\n",
    "    add_special_tokens = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1ca22ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1607, 30, 273, 274, 598, 307, 225, 3972, 18, 822, 286, 449, 441, 35, 206, 203, 2115, 30, 893, 5, 206, 203, 1607, 30, 273, 419, 933, 286, 606, 1677]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_sources.data['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "28756f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextSummarizationDataset(tokenized_train_sources.data['input_ids'], tokenized_train_targets.data['input_ids'], tokenized_train_labels.data['input_ids'])\n",
    "val_ds = TextSummarizationDataset(tokenized_val_sources.data['input_ids'], tokenized_val_targets.data['input_ids'], tokenized_val_labels.data['input_ids'])\n",
    "test_ds = TextSummarizationDataset(tokenized_test_sources.data['input_ids'], tokenized_test_targets.data['input_ids'], tokenized_test_labels.data['input_ids'])\n",
    "\n",
    "# train_dataloader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True)\n",
    "# val_dataloader = DataLoader(val_ds, batch_size = BATCH_SIZE)\n",
    "# test_dataloader = DataLoader(test_ds, batch_size = BATCH_SIZE)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True, collate_fn = partial(list_padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))\n",
    "val_dataloader = DataLoader(val_ds, batch_size = BATCH_SIZE, collate_fn = partial(list_padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))\n",
    "test_dataloader = DataLoader(test_ds, batch_size = BATCH_SIZE, collate_fn = partial(list_padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f5508a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([850, 30, 2291, 326, 928, 317, 275, 1089, 2090, 996, 35, 206, 203, 3306, 30, 273, 413, 333, 468, 360, 206, 203, 2043, 389, 30, 610, 16, 275, 704, 1026, 1492, 372, 3323, 350], [0, 460, 704, 1026, 1588, 1565, 372, 285, 330, 1274, 3497, 16, 360, 2204, 3836, 298, 309, 1187, 75, 389, 413, 333, 362, 280, 928, 317, 1089, 366, 529, 319, 996, 18]) [1142, 704, 1026, 1588, 1565, 372, 285, 330, 1274, 3497, 16, 360, 2204, 3836, 298, 309, 1187, 75, 389, 413, 333, 362, 280, 928, 317, 1089, 366, 529, 319, 996, 18, 225, 1]\n",
      "([1996, 30, 3040, 16, 372, 286, 476, 324, 441, 284, 93, 71, 1587, 655, 35, 206, 203, 703, 30, 504, 383, 655, 377, 798, 606, 35, 206, 203, 1336, 30, 689, 606, 323, 820, 16, 798, 931, 815, 1347, 35, 206, 203, 1996, 30, 360, 286, 372, 1428, 1293, 655, 1175, 944, 286, 981, 660, 1730, 338, 35, 206, 203, 703, 30, 272, 635, 482, 280, 275, 3874, 707, 16, 1352, 1057, 35, 754, 354, 40, 206, 203, 1336, 30, 984, 338, 495, 354, 40, 417, 263, 284, 746, 71, 332, 2175], [0, 1306, 309, 3577, 372, 1428, 482, 280, 275, 3874, 707, 655, 18, 1868, 263, 1122, 1274, 16, 478, 381, 333, 371, 284, 93, 71, 1587, 375, 1187, 821, 18]) [703, 309, 3577, 372, 1428, 482, 280, 275, 3874, 707, 655, 18, 1868, 263, 1122, 1274, 16, 478, 381, 333, 371, 284, 93, 71, 1587, 375, 1187, 821, 18, 225, 1]\n",
      "tensor([[ 850,   30, 2291,  326,  928,  317,  275, 1089, 2090,  996,   35,  206,\n",
      "          203, 3306,   30,  273,  413,  333,  468,  360,  206,  203, 2043,  389,\n",
      "           30,  610,   16,  275,  704, 1026, 1492,  372, 3323,  350,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2],\n",
      "        [1996,   30, 3040,   16,  372,  286,  476,  324,  441,  284,   93,   71,\n",
      "         1587,  655,   35,  206,  203,  703,   30,  504,  383,  655,  377,  798,\n",
      "          606,   35,  206,  203, 1336,   30,  689,  606,  323,  820,   16,  798,\n",
      "          931,  815, 1347,   35,  206,  203, 1996,   30,  360,  286,  372, 1428,\n",
      "         1293,  655, 1175,  944,  286,  981,  660, 1730,  338,   35,  206,  203,\n",
      "          703,   30,  272,  635,  482,  280,  275, 3874,  707,   16, 1352, 1057,\n",
      "           35,  754,  354,   40,  206,  203, 1336,   30,  984,  338,  495,  354,\n",
      "           40,  417,  263,  284,  746,   71,  332, 2175]])\n",
      "tensor([[   0,  460,  704, 1026, 1588, 1565,  372,  285,  330, 1274, 3497,   16,\n",
      "          360, 2204, 3836,  298,  309, 1187,   75,  389,  413,  333,  362,  280,\n",
      "          928,  317, 1089,  366,  529,  319,  996,   18],\n",
      "        [   0, 1306,  309, 3577,  372, 1428,  482,  280,  275, 3874,  707,  655,\n",
      "           18, 1868,  263, 1122, 1274,   16,  478,  381,  333,  371,  284,   93,\n",
      "           71, 1587,  375, 1187,  821,   18,    2,    2]])\n",
      "tensor([[1142,  704, 1026, 1588, 1565,  372,  285,  330, 1274, 3497,   16,  360,\n",
      "         2204, 3836,  298,  309, 1187,   75,  389,  413,  333,  362,  280,  928,\n",
      "          317, 1089,  366,  529,  319,  996,   18,  225,    1],\n",
      "        [ 703,  309, 3577,  372, 1428,  482,  280,  275, 3874,  707,  655,   18,\n",
      "         1868,  263, 1122, 1274,   16,  478,  381,  333,  371,  284,   93,   71,\n",
      "         1587,  375, 1187,  821,   18,  225,    1,    2,    2]])\n"
     ]
    }
   ],
   "source": [
    "(source, target), label = next(iter(train_dataloader))\n",
    "print(source)\n",
    "print(target)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc9e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "<class 'tokenizers.Encoding'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_train_sources))\n",
    "print(type(tokenized_train_sources[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "14732\n",
      "[1607, 30, 273, 274, 598, 307, 225, 3972, 18, 822, 286, 449, 441, 35, 206, 203, 2115, 30, 893, 5, 206, 203, 1607, 30, 273, 419, 933, 286, 606, 1677]\n",
      "['Amanda', ':', 'ĠI', 'Ġb', 'ak', 'ed', 'Ġ', 'Ġcookies', '.', 'ĠDo', 'Ġyou', 'Ġwant', 'Ġsome', '?', 'č', 'Ċ', 'Jerry', ':', 'ĠSure', '!', 'č', 'Ċ', 'Amanda', ':', 'ĠI', \"'ll\", 'Ġbring', 'Ġyou', 'Ġtomorrow', 'Ġ:-)']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_sources.data.keys())\n",
    "print(len(tokenized_train_sources.data['input_ids']))\n",
    "print(tokenized_train_sources.data['input_ids'][0])\n",
    "print(tokenized_train_sources.encodings[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e2f9779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Georg', 'ia', ':', 'Ġare', 'Ġyou', 'Ġready', 'Ġfor', 'Ġhotel', 'Ġhun', 'ting', '?', 'ĠWe', 'Ġneed', 'Ġto', 'Ġbook', 'Ġsomething', 'Ġfinally', 'Ġfor', 'ĠL', 'is', 'b', 'on', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġsure', 'Ġwe', 'Ġcan', 'Ġgo', 'Ġon', ',', 'Ġshow', 'Ġme', 'Ġwhat', 'Ġyou', 'Ġfound', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġ<', 'file', '_', 'photo', '>', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġn', 'ah', '...', 'Ġit', 'Ġlooks', 'Ġlike', 'Ġan', 'Ġold', 'Ġlady', \"'s\", 'Ġroom', 'Ġlol', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġ<', 'file', '_', 'photo', '>', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġthat', \"'s\", 'Ġbetter', '...', 'Ġbut', 'Ġthe', 'Ġbed', 'Ġdoesn', \"'t\", 'Ġlook', 'Ġvery', 'Ġcomfort', 'able', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġi', 'Ġkind', 'Ġof', 'Ġlike', 'Ġit', 'Ġand', 'Ġit', \"'s\", 'Ġreally', 'Ġclose', 'Ġto', 'Ġthe', 'Ġcity', 'Ġc', 'en', 'ter', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġshow', 'Ġme', 'Ġthe', 'Ġothers', 'Ġplease', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġ<', 'file', '_', 'photo', '>', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġn', 'ah', '...', 'Ġthis', 'Ġone', 'Ġsucks', 'Ġtoo', ',', 'Ġlook', 'Ġat', 'Ġthose', 'Ġhorrible', 'Ġc', 'urt', 'ains', 'Ġ', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġaff', 'ĠJul', 'ie', 'Ġyou', 'Ġare', 'Ġsuch', 'Ġa', 'Ġpr', 'inc', 'ess', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġi', 'Ġjust', 'Ġwant', 'Ġto', 'Ġbe', 'Ġcomfort', 'able', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġcome', 'Ġon', ',', 'Ġstop', 'Ġwh', 'in', 'ing', 'Ġyou', 'Ġknow', 'Ġwe', 'Ġare', 'Ġon', 'Ġa', 'Ġbud', 'get', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġwell', 'Ġhope', 'fully', 'Ġwe', 'Ġcan', 'Ġfind', 'Ġsomething', 'Ġthat', \"'s\", 'Ġdec', 'ent', 'Ġright', '?', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġi', 'Ġdid', 'Ġshow', 'Ġyou', 'Ġdec', 'ent', 'Ġbut', 'Ġyou', 'Ġwant', 'Ġa', 'ĠMar', 'ri', 'ott', 'Ġor', 'Ġsomething', 'Ġ:/', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġok', 'Ġok', 'Ġdon', \"'t\", 'Ġget', 'Ġangry', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġwe', 'Ġneed', 'Ġto', 'Ġdecide', 'Ġtoday', ',', 'Ġthe', 'Ġlonger', 'Ġwe', 'Ġwait', 'Ġthe', 'Ġh', 'ig', 'her', 'Ġthe', 'Ġpr', 'ices', 'Ġget', 'Ġ', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġok', 'Ġhow', 'Ġabout', 'Ġwe', 'Ġget', 'Ġthe', 'Ġsecond', 'Ġone', 'Ġthen', '?', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġok', 'Ġgive', 'Ġme', 'Ġa', 'Ġsecond', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġsure', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġaff', 'ff', 'Ġsomeone', 'Ġalready', 'Ġbooked', 'Ġit', 'Ġ:/', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġoh', 'Ġmy', 'Ġgod', '...', 'Ġthat', \"'s\", 'Ġb', 'ull', 'sh', 'it', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġwhatever', ',', 'Ġcan', 'Ġwe', 'Ġjust', 'Ġgo', 'Ġwith', 'Ġthe', 'Ġth', 'ird', 'Ġone', '?', 'Ġit', 'Ġlooks', 'Ġnice', 'Ġover', 'all', 'Ġand', 'Ġmaybe', 'Ġthey', 'Ġchanged', 'Ġthe', 'Ġc', 'urt', 'ains', 'Ġalready', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġok', ',', 'Ġfine', 'Ġwith', 'Ġme', 'č', 'Ċ', 'Georg', 'ia', ':', 'Ġdone', 'Ġ-', 'Ġbooked', 'Ġ:)', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġperfect', '!!!', 'Ġcan', \"'t\", 'Ġwait', 'Ġto', 'Ġgo', '!!!', 'č', 'Ċ', 'Georg', 'ia', ':', 'ĠI', 'Ġknow', 'Ġ:', 'D', 'Ġwe', 'Ġwill', 'Ġhave', 'Ġthe', 'Ġbest', 'Ġtime', 'č', 'Ċ', 'Jul', 'i', 'ette', ':', 'Ġtime', 'Ġof', 'Ġour', 'Ġlives', 'Ġ<', '3', 'Ġ', '!']\n",
      "Encoding(num_tokens=30, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "print(train_encoded[14731].tokens)\n",
    "print(train_encoded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
