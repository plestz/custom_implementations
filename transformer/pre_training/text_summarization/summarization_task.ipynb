{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f8197d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "48e3952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the parent directory (i.e. project root)\n",
    "project_root = Path().resolve().parent.parent \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenization.byte_pair_encoding.get_tokenizers import train_and_save_tokenizer_for, load_tokenizer_from\n",
    "\n",
    "from pre_training.text_summarization.dataset import TextSummarizationDataset\n",
    "\n",
    "from src.embedding import CustomEmbedding\n",
    "from src.transformer import EncoderDecoderTransformer\n",
    "from src.utils import padding_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "81921cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_DATA_PATH = '../../data/SAMSum/'\n",
    "\n",
    "BPE_IN_PATH = '../../data/SAMSum/train_summary_and_dialogue.txt'\n",
    "BPE_OUT_PATH = '../../tokenization/trained_tokenizers/SAMSum_BPE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0a22ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT_WINDOW = 100\n",
    "\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "73f11a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(DF_DATA_PATH + 'train_df.json', orient = 'records', lines = True)\n",
    "val_df = pd.read_json(DF_DATA_PATH + 'val_df.json', orient = 'records', lines = True)\n",
    "test_df = pd.read_json(DF_DATA_PATH + 'test_df.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "319eede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The pad token index is 2.\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = train_and_save_tokenizer_for(in_file_paths = [BPE_IN_PATH], out_file_dir_path = BPE_OUT_PATH, vocab_size = 4_000)\n",
    "pretrained_bpe_tokenizer = load_tokenizer_from(dir_path = BPE_OUT_PATH, model_max_length = 10000)\n",
    "\n",
    "PAD_TOKEN_IDX = pretrained_bpe_tokenizer.pad_token_id\n",
    "print(f'The pad token index is {PAD_TOKEN_IDX}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8a000cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a max_context_window of 100...\n",
      "The number of training samples went from 14732 to 5580\n",
      "The number of validation samples went from 818 to 325\n",
      "The number of test samples went from 819 to 308\n"
     ]
    }
   ],
   "source": [
    "FILTER_tokenized_train_sources = pretrained_bpe_tokenizer(\n",
    "    train_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_train_targets = pretrained_bpe_tokenizer(\n",
    "    train_df['summary'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_val_sources = pretrained_bpe_tokenizer(\n",
    "    val_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_val_targets = pretrained_bpe_tokenizer(\n",
    "    val_df['summary'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_test_sources = pretrained_bpe_tokenizer(\n",
    "    test_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_test_targets = pretrained_bpe_tokenizer(\n",
    "    test_df['summary'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "valid_src_train_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_train_sources.data['input_ids']])\n",
    "valid_src_val_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_val_sources.data['input_ids']])\n",
    "valid_src_test_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_test_sources.data['input_ids']])\n",
    "\n",
    "valid_tgt_train_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW - 1 for example in FILTER_tokenized_train_targets.data['input_ids']])\n",
    "valid_tgt_val_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW - 1 for example in FILTER_tokenized_val_targets.data['input_ids']])\n",
    "valid_tgt_test_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW - 1 for example in FILTER_tokenized_test_targets.data['input_ids']])\n",
    "\n",
    "valid_train_df = train_df.iloc[valid_src_train_indices & valid_tgt_train_indices]\n",
    "valid_val_df = val_df.iloc[valid_src_val_indices & valid_tgt_val_indices]\n",
    "valid_test_df = test_df.iloc[valid_src_test_indices & valid_tgt_test_indices]\n",
    "\n",
    "print(f'With a max_context_window of {MAX_CONTEXT_WINDOW}...')\n",
    "print(f'The number of training samples went from {train_df.shape[0]} to {valid_train_df.shape[0]}')\n",
    "print(f'The number of validation samples went from {val_df.shape[0]} to {valid_val_df.shape[0]}')\n",
    "print(f'The number of test samples went from {test_df.shape[0]} to {valid_test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "686aeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sources = pretrained_bpe_tokenizer(\n",
    "    valid_train_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_train_targets = pretrained_bpe_tokenizer(\n",
    "    ('<SOS> ' + valid_train_df['summary']).tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_train_labels = pretrained_bpe_tokenizer(\n",
    "    (valid_train_df['summary'] + ' <EOS>').tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_val_sources = pretrained_bpe_tokenizer(\n",
    "    valid_val_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_val_targets = pretrained_bpe_tokenizer(\n",
    "    ('<SOS> ' + valid_val_df['summary']).tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_val_labels = pretrained_bpe_tokenizer(\n",
    "    (valid_val_df['summary'] + ' <EOS>').tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_test_sources = pretrained_bpe_tokenizer(\n",
    "    valid_test_df['dialogue'].tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_test_targets = pretrained_bpe_tokenizer(\n",
    "    ('<SOS> ' + valid_test_df['summary']).tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_test_labels = pretrained_bpe_tokenizer(\n",
    "    (valid_test_df['summary'] + ' <EOS>').tolist(),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "28756f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextSummarizationDataset(tokenized_train_sources.data['input_ids'], tokenized_train_targets.data['input_ids'], tokenized_train_labels.data['input_ids'])\n",
    "val_ds = TextSummarizationDataset(tokenized_val_sources.data['input_ids'], tokenized_val_targets.data['input_ids'], tokenized_val_labels.data['input_ids'])\n",
    "test_ds = TextSummarizationDataset(tokenized_test_sources.data['input_ids'], tokenized_test_targets.data['input_ids'], tokenized_test_labels.data['input_ids'])\n",
    "\n",
    "# NOTE: Option to use HuggingFace DataCollatorWithPadding : requires changing TextSummarizationDataset __getitem__\n",
    "train_dataloader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))\n",
    "val_dataloader = DataLoader(val_ds, batch_size = BATCH_SIZE, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))\n",
    "test_dataloader = DataLoader(test_ds, batch_size = BATCH_SIZE, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f5508a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1665, 30, 1798, 16, 448, 314, 263, 1676, 302, 401, 327, 3329, 588, 2071, 287, 313, 365, 81, 18, 206, 203, 1665, 30, 1164, 286, 2296, 476, 324, 896, 1675, 80, 299, 390, 16, 286, 381, 466, 263, 1170, 1457, 18, 206, 203, 1665, 30, 487, 351, 527, 709, 523, 286, 518, 837, 2547, 307, 440, 1043, 280, 353, 304, 912, 275, 1232, 337, 275, 582, 27, 436, 18, 206, 203, 867, 30, 273, 419, 898, 304, 586, 273, 400, 612, 365, 403, 756, 18, 206, 203, 867, 30, 812, 5, 206, 203, 1665, 30, 610, 811, 18], [0, 421, 301, 2446, 1798, 346, 316, 585, 2547, 476, 324, 275, 1675, 80, 299, 390, 317, 1664, 280, 466, 263, 1170, 1457, 327, 3329, 588, 2071, 287, 313, 18, 1798, 351, 898, 304, 586, 316, 314, 612, 365, 538, 756, 18]) [1665, 2446, 1798, 346, 316, 585, 2547, 476, 324, 275, 1675, 80, 299, 390, 317, 1664, 280, 466, 263, 1170, 1457, 327, 3329, 588, 2071, 287, 313, 18, 1798, 351, 898, 304, 586, 316, 314, 612, 365, 538, 756, 18, 225, 1]\n",
      "([748, 30, 496, 263, 311, 858, 281, 206, 203, 748, 30, 263, 1173, 501, 206, 203, 748, 30, 2060, 323, 280, 2022, 206, 203, 748, 30, 353, 366, 362, 501, 35, 206, 203, 3908, 30, 383, 537, 206, 203, 3908, 30, 377, 273, 468, 409, 2480, 362, 206, 203, 3908, 30, 698, 531, 206, 203, 748, 30, 499, 16, 2509], [0, 1769, 1144, 263, 1173, 311, 858, 281, 18, 409, 2480, 1063, 362, 501, 18, 225]) [748, 1144, 263, 1173, 311, 858, 281, 18, 409, 2480, 1063, 362, 501, 18, 225, 225, 1]\n",
      "tensor([[1665,   30, 1798,   16,  448,  314,  263, 1676,  302,  401,  327, 3329,\n",
      "          588, 2071,  287,  313,  365,   81,   18,  206,  203, 1665,   30, 1164,\n",
      "          286, 2296,  476,  324,  896, 1675,   80,  299,  390,   16,  286,  381,\n",
      "          466,  263, 1170, 1457,   18,  206,  203, 1665,   30,  487,  351,  527,\n",
      "          709,  523,  286,  518,  837, 2547,  307,  440, 1043,  280,  353,  304,\n",
      "          912,  275, 1232,  337,  275,  582,   27,  436,   18,  206,  203,  867,\n",
      "           30,  273,  419,  898,  304,  586,  273,  400,  612,  365,  403,  756,\n",
      "           18,  206,  203,  867,   30,  812,    5,  206,  203, 1665,   30,  610,\n",
      "          811,   18],\n",
      "        [ 748,   30,  496,  263,  311,  858,  281,  206,  203,  748,   30,  263,\n",
      "         1173,  501,  206,  203,  748,   30, 2060,  323,  280, 2022,  206,  203,\n",
      "          748,   30,  353,  366,  362,  501,   35,  206,  203, 3908,   30,  383,\n",
      "          537,  206,  203, 3908,   30,  377,  273,  468,  409, 2480,  362,  206,\n",
      "          203, 3908,   30,  698,  531,  206,  203,  748,   30,  499,   16, 2509,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2]])\n",
      "tensor([[   0,  421,  301, 2446, 1798,  346,  316,  585, 2547,  476,  324,  275,\n",
      "         1675,   80,  299,  390,  317, 1664,  280,  466,  263, 1170, 1457,  327,\n",
      "         3329,  588, 2071,  287,  313,   18, 1798,  351,  898,  304,  586,  316,\n",
      "          314,  612,  365,  538,  756,   18],\n",
      "        [   0, 1769, 1144,  263, 1173,  311,  858,  281,   18,  409, 2480, 1063,\n",
      "          362,  501,   18,  225,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2]])\n",
      "tensor([[1665, 2446, 1798,  346,  316,  585, 2547,  476,  324,  275, 1675,   80,\n",
      "          299,  390,  317, 1664,  280,  466,  263, 1170, 1457,  327, 3329,  588,\n",
      "         2071,  287,  313,   18, 1798,  351,  898,  304,  586,  316,  314,  612,\n",
      "          365,  538,  756,   18,  225,    1],\n",
      "        [ 748, 1144,  263, 1173,  311,  858,  281,   18,  409, 2480, 1063,  362,\n",
      "          501,   18,  225,  225,    1,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
      "            2,    2,    2,    2,    2,    2]])\n"
     ]
    }
   ],
   "source": [
    "(source, target), label = next(iter(train_dataloader))\n",
    "print(source)\n",
    "print(target)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399cf5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
