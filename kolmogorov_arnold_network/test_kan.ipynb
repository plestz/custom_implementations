{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import KAN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "data = fetch_california_housing()\n",
    "column_names = data['feature_names']\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "print(column_names) # y = 'MedInc'\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSubset(Enum):\n",
    "    TRAIN = 1\n",
    "    TEST = 2\n",
    "\n",
    "class CaliforniaHousingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Represents the California Housing Price dataset from scikit-learn.\n",
    "    \"\"\"\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, split: DSubset, train_size = 0.8, transform = None, target_transform = None, standard_scaler: StandardScaler = None):\n",
    "        \"\"\"\n",
    "        Initializes dataset, properly scaling X for training.\n",
    "        \"\"\"\n",
    "        num_train = int(train_size * y.shape[0])\n",
    "\n",
    "        if split == DSubset.TRAIN:\n",
    "            self.X = X[:num_train]\n",
    "            self.y = y[:num_train]\n",
    "        else:\n",
    "            self.X = X[num_train:]\n",
    "            self.y = y[num_train:]\n",
    "\n",
    "        if split == DSubset.TRAIN:\n",
    "            assert not standard_scaler\n",
    "            self.ss = StandardScaler()\n",
    "            self.X = self.ss.fit_transform(self.X)\n",
    "        else:\n",
    "            assert standard_scaler\n",
    "            self.X = standard_scaler.transform(self.X)\n",
    "\n",
    "        self.X = torch.tensor(self.X).float()\n",
    "        self.y = torch.tensor(self.y).float()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Provides the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Provides the instance and label at the corresponding index within\n",
    "        the dataset.\n",
    "        \"\"\"\n",
    "        instance, label = self.X[idx], self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            instance = self.transform(instance)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return instance, label\n",
    "\n",
    "    def get_standard_scaler(self) -> StandardScaler:\n",
    "        return self.ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "train_dataset = CaliforniaHousingDataset(X, y, DSubset.TRAIN, train_size = TRAIN_SIZE)\n",
    "test_dataset = CaliforniaHousingDataset(X, y, DSubset.TEST, train_size = TRAIN_SIZE, standard_scaler = train_dataset.get_standard_scaler())\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_epoch(dataloader: DataLoader, model: nn.Module, loss_fn, optimizer: torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Runs one full epoch of training on model.\n",
    "\n",
    "    Args:\n",
    "        dataloader -- The DataLoader through which to produce instances.\n",
    "        model -- The model to be used for label prediction on instances.\n",
    "        loss_fn -- The loss function, for backpropagation\n",
    "        optimizer -- The optimizer, for reducing loss\n",
    "\n",
    "    Returns:\n",
    "        average_epoch_loss -- The model's loss this epoch, averaged by the number of instances in the dataset\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Forward\n",
    "        pred = model(X)\n",
    "        batch_loss = loss_fn(pred.squeeze(1), y)\n",
    "\n",
    "        # Log\n",
    "        epoch_loss += batch_loss.item()\n",
    "\n",
    "        # Backpropagate\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Display\n",
    "        # print(f'Batch {i + 1} Loss: {batch_loss}')\n",
    "\n",
    "    average_epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "\n",
    "    return average_epoch_loss\n",
    "\n",
    "def run_test_epoch(dataloader: DataLoader, model: nn.Module, loss_fn):\n",
    "    \"\"\"\n",
    "    Runs one full dataset-worth of testing on model.\n",
    "\n",
    "    Args:\n",
    "        dataloader -- The DataLoader through which to produce instances.\n",
    "        model -- The model to be used for label prediction on instances.\n",
    "        loss_fn -- The loss function, for measuring generalizability\n",
    "\n",
    "    Returns:\n",
    "        average_epoch_loss -- The model's loss this epoch, averaged by the number of instances in the dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            # Forward\n",
    "            pred = model(X)\n",
    "            batch_loss = loss_fn(pred.squeeze(1), y)\n",
    "\n",
    "            # Log\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "    average_epoch_loss = epoch_loss / len(dataloader.dataset)\n",
    "\n",
    "    return average_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN(len(column_names), 1)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3, momentum = 0.9, weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train Loss:    0.0279 | Test Loss:    0.0212\n",
      "Epoch   2 | Train Loss:    0.0176 | Test Loss:    0.0177\n",
      "Epoch   3 | Train Loss:    0.0151 | Test Loss:    0.0170\n",
      "Epoch   4 | Train Loss:    0.0141 | Test Loss:    0.0166\n",
      "Epoch   5 | Train Loss:    0.0130 | Test Loss:    0.0173\n",
      "Epoch   6 | Train Loss:    0.0121 | Test Loss:    0.0143\n",
      "Epoch   7 | Train Loss:    0.0117 | Test Loss:    0.0171\n",
      "Epoch   8 | Train Loss:    0.0114 | Test Loss:    0.0133\n",
      "Epoch   9 | Train Loss:    0.0112 | Test Loss:    0.0146\n",
      "Epoch  10 | Train Loss:    0.0110 | Test Loss:    0.0145\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    avg_train_loss = run_train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "    avg_test_loss = run_test_epoch(test_dataloader, model, loss_fn)\n",
    "\n",
    "    print(f'Epoch {i + 1:>3} | Train Loss: {avg_train_loss:>9.4f} | Test Loss: {avg_test_loss:>9.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
