{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the parent directory (i.e. project root)\n",
    "project_root = Path().resolve().parent.parent \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "from pre_training_encoder_decoder.sort_integer_lists.dataset import RandomIntegerDataset\n",
    "\n",
    "from src.embedding import CustomEmbedding\n",
    "from src.transformer import EncoderDecoderTransformer\n",
    "from src.utils import padding_collate_fn\n",
    "\n",
    "from src.train_utils import run_train_epoch\n",
    "from src.validation_utils import run_gold_validation_loop, run_autoregressive_validation_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_real_tokens = 10\n",
    "PAD_TOKEN_IDX = n_real_tokens\n",
    "SOS_TOKEN_IDX = n_real_tokens + 1\n",
    "EOS_TOKEN_IDX = n_real_tokens + 2\n",
    "vocab_size = n_real_tokens + 3\n",
    "D_MODEL = 64\n",
    "\n",
    "embeddings = CustomEmbedding(vocab_size, d_model = D_MODEL) # 3 = PAD, SOS, EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT_WINDOW = 50\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MIN_SEQ_LEN = 2\n",
    "MAX_SEQ_LEN = min(20, MAX_CONTEXT_WINDOW)\n",
    "\n",
    "NUM_TRAINING_SEQUENCES = 10000\n",
    "NUM_VALIDATION_SEQUENCES = 1000\n",
    "\n",
    "VOCAB = [i for i in range(n_real_tokens)] # does not include SOS, EOS, PAD\n",
    "\n",
    "VOCAB_MAP = dict()\n",
    "\n",
    "for i, token in enumerate(VOCAB):\n",
    "    VOCAB_MAP[i] = token\n",
    "VOCAB_MAP[len(VOCAB_MAP)] = '<PAD>'\n",
    "VOCAB_MAP[len(VOCAB_MAP) + 1] = '<SOS>'\n",
    "VOCAB_MAP[len(VOCAB_MAP) + 2] = '<EOS>'\n",
    "\n",
    "train_rand_ds = RandomIntegerDataset(MIN_SEQ_LEN, MAX_SEQ_LEN, NUM_TRAINING_SEQUENCES, VOCAB)\n",
    "train_dataloader = DataLoader(train_rand_ds, batch_size = BATCH_SIZE, shuffle = True, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))\n",
    "\n",
    "val_rand_ds = RandomIntegerDataset(MIN_SEQ_LEN, MAX_SEQ_LEN, NUM_VALIDATION_SEQUENCES, VOCAB)\n",
    "val_dataloader = DataLoader(val_rand_ds, batch_size = BATCH_SIZE, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  6,  4,  ..., 10, 10, 10],\n",
      "        [ 1,  3,  6,  ..., 10, 10, 10],\n",
      "        [ 6,  8,  5,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 1,  2, 10,  ..., 10, 10, 10],\n",
      "        [ 1,  3,  1,  ...,  3,  7, 10],\n",
      "        [ 3,  8,  2,  ..., 10, 10, 10]])\n",
      "tensor([[11,  4,  6,  ..., 10, 10, 10],\n",
      "        [11,  1,  3,  ..., 10, 10, 10],\n",
      "        [11,  5,  6,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [11,  1,  2,  ..., 10, 10, 10],\n",
      "        [11,  0,  0,  ...,  7,  9, 10],\n",
      "        [11,  0,  0,  ..., 10, 10, 10]])\n",
      "tensor([[ 4,  6,  7,  ..., 10, 10, 10],\n",
      "        [ 1,  3,  6,  ..., 10, 10, 10],\n",
      "        [ 5,  6,  6,  ..., 10, 10, 10],\n",
      "        ...,\n",
      "        [ 1,  2, 12,  ..., 10, 10, 10],\n",
      "        [ 0,  0,  1,  ...,  9, 12, 10],\n",
      "        [ 0,  0,  1,  ..., 10, 10, 10]])\n"
     ]
    }
   ],
   "source": [
    "input, label = next(iter(train_dataloader))\n",
    "print(input[0])\n",
    "print(input[1])\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul/opt/miniconda3/envs/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_IDX, reduction = 'sum')\n",
    "\n",
    "model = EncoderDecoderTransformer(\n",
    "                    embeddings = embeddings, \n",
    "                    vocab_size = vocab_size, \n",
    "                    d_model = D_MODEL, \n",
    "                    num_attention_heads = 4, \n",
    "                    num_encoder_layers = 2, \n",
    "                    num_decoder_layers = 2, \n",
    "                    dim_feedforward = 32, \n",
    "                    dropout = 0.0,\n",
    "                    max_context_window = MAX_CONTEXT_WINDOW,\n",
    "                    use_pre_lnorm = True)\n",
    "\n",
    "optim = torch.optim.SGD(params = model.parameters(), lr = 1e-4, momentum = 0.9, weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:07<00:00, 20.34it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 60.96it/s]\n",
      "100%|██████████| 157/157 [00:07<00:00, 21.45it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 60.75it/s]\n",
      "100%|██████████| 157/157 [00:07<00:00, 21.20it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 55.14it/s]\n",
      "100%|██████████| 157/157 [00:07<00:00, 19.92it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 62.05it/s]\n",
      "100%|██████████| 157/157 [00:07<00:00, 22.38it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 62.77it/s]\n",
      "100%|██████████| 157/157 [00:07<00:00, 22.07it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 58.50it/s]\n",
      "100%|██████████| 157/157 [00:07<00:00, 21.76it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 62.95it/s]\n",
      "100%|██████████| 157/157 [00:06<00:00, 22.51it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 54.11it/s]\n",
      "100%|██████████| 157/157 [00:07<00:00, 20.12it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 58.94it/s]\n",
      "100%|██████████| 157/157 [00:07<00:00, 20.69it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 43.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.073650161743164, 1.9356828886032105, 1.0216213793754578, 0.4279505677700043, 0.2879812387943268, 0.7921349413394928, 0.14866112573742868, 0.36519185576438906, 0.3517247172355652, 0.17542103811502457]\n",
      "[0.0011, 0.0125, 0.024, 0.0386, 0.0425, 0.0294, 0.0479, 0.0448, 0.0446, 0.045]\n",
      "[0.4569874532835024, 0.53721192002442, 0.5527929285223891, 0.5631155050003817, 0.5651566315628815, 0.5566821681786994, 0.5673946645294252, 0.5643936503090895, 0.5643636086326547, 0.5663416075650118]\n",
      "\n",
      "[3.8889976806640627, 1.5424590377807617, 0.9636995162963867, 0.38659665870666504, 0.6576802501678467, 0.48469909572601316, 0.05351661467552185, 4.776434967041015, 0.040406386137008664, 0.03617469418048859]\n",
      "[0.001, 0.003, 0.016, 0.03, 0.018, 0.03, 0.05, 0.002, 0.051, 0.052]\n",
      "[0.4915456629728697, 0.534772640427971, 0.5437523882307986, 0.5546904852884983, 0.5506782575468093, 0.5544516622086358, 0.5595147115017195, 0.51920137562094, 0.559753534581582, 0.5598012991975545]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "training_losses = list()\n",
    "training_sequence_accuracies = list()\n",
    "training_token_accuracies = list()\n",
    "\n",
    "gold_validation_losses = list()\n",
    "gold_validation_sequence_accuracies = list()\n",
    "gold_validation_token_accuracies = list()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # print(f'Running epoch {i+1}...')\n",
    "\n",
    "    training_loss, training_sequence_accuracy, training_token_accuracy = run_train_epoch(train_dataloader, model, loss_fn, optim, calculate_sequence_accuracy = True, calculate_token_accuracy = True)\n",
    "\n",
    "    training_losses.append(training_loss)\n",
    "    training_sequence_accuracies.append(training_sequence_accuracy)\n",
    "    training_token_accuracies.append(training_token_accuracy)\n",
    "\n",
    "    gold_val_loss, gold_val_sequence_accuracy, gold_val_token_accuracy = run_gold_validation_loop(val_dataloader, model, loss_fn, calculate_sequence_accuracy = True, calculate_token_accuracy = True)\n",
    "    \n",
    "    gold_validation_losses.append(gold_val_loss)\n",
    "    gold_validation_sequence_accuracies.append(gold_val_sequence_accuracy)\n",
    "    gold_validation_token_accuracies.append(gold_val_token_accuracy)\n",
    "\n",
    "print(training_losses)\n",
    "print(training_sequence_accuracies)\n",
    "print(training_token_accuracies)\n",
    "\n",
    "print()\n",
    "\n",
    "print(gold_validation_losses)\n",
    "print(gold_validation_sequence_accuracies)\n",
    "print(gold_validation_token_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:01<00:01,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sequence 1:\n",
      "['0' '4' '6' '7' '8' '8' '9' '9' '9' '9' '9']\n",
      "['0' '4' '6' '7' '8' '8' '9' '9' '9' '9']\n",
      "Source:              ['9' '9' '4' '9' '8' '6' '0' '8' '7' '9' '9' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "Predicted Target:    ['<SOS>' '0' '4' '6' '7' '8' '8' '9' '9' '9' '9' '<EOS>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "\n",
      "Incorrect Sequence 2:\n",
      "['0' '0' '1' '1' '1' '8' '9']\n",
      "['0' '0' '1' '1' '1' '1' '8' '9']\n",
      "Source:              ['9' '0' '1' '1' '1' '8' '0' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "Predicted Target:    ['<SOS>' '0' '0' '1' '1' '1' '1' '8' '9' '<EOS>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [00:02<00:00,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sequence 3:\n",
      "['0' '0' '1' '1' '1' '1' '1' '2' '2' '2' '2' '2' '2' '7' '7' '9' '9' '9']\n",
      "['0' '0' '1' '1' '1' '1' '1' '2' '2' '2' '2' '2' '7' '7' '9' '9' '9']\n",
      "Source:              ['9' '0' '7' '0' '2' '2' '2' '1' '7' '1' '1' '9' '2' '2' '2' '1' '9' '1'\n",
      " '<PAD>' '<PAD>']\n",
      "Predicted Target:    ['<SOS>' '0' '0' '1' '1' '1' '1' '1' '2' '2' '2' '2' '2' '7' '7' '9' '9'\n",
      " '9' '<EOS>' '<PAD>' '<PAD>' '<PAD>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Sequence 4:\n",
      "['2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '3' '3' '3' '4' '6' '7' '8' '8']\n",
      "['2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '3' '3' '3' '4' '6' '7' '8'\n",
      " '8']\n",
      "Source:              ['7' '2' '2' '8' '3' '2' '3' '2' '2' '3' '4' '2' '6' '8' '2' '2' '2' '2'\n",
      " '<PAD>' '<PAD>']\n",
      "Predicted Target:    ['<SOS>' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '3' '3' '3' '4' '6'\n",
      " '7' '8' '8' '<EOS>' '<PAD>']\n",
      "\n",
      "0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "special_token_idxs = {\n",
    "    'SOS_TOKEN_IDX': SOS_TOKEN_IDX,\n",
    "    'EOS_TOKEN_IDX': EOS_TOKEN_IDX,\n",
    "    'PAD_TOKEN_IDX': PAD_TOKEN_IDX\n",
    "}\n",
    "\n",
    "acc = run_autoregressive_validation_loop(val_dataloader, model, VOCAB_MAP, special_token_idxs, MAX_CONTEXT_WINDOW)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
