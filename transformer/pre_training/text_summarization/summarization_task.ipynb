{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f8197d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "48e3952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the parent directory (i.e. project root)\n",
    "project_root = Path().resolve().parent.parent \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "from tokenization.byte_pair_encoding.get_tokenizers import train_and_save_tokenizer_for, load_tokenizer_from\n",
    "\n",
    "from pre_training.text_summarization.dataset import TextSummarizationDataset\n",
    "\n",
    "from src.embedding import CustomEmbedding\n",
    "from src.transformer import EncoderDecoderTransformer\n",
    "from src.utils import padding_collate_fn\n",
    "\n",
    "from src.train_utils import run_train_epoch\n",
    "from src.validation_utils import run_gold_validation_loop, run_autoregressive_validation_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "81921cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_DATA_PATH = '../../data/SAMSum/'\n",
    "\n",
    "BPE_IN_PATH = '../../data/SAMSum/train_summary_and_dialogue.txt'\n",
    "BPE_OUT_PATH = '../../tokenization/trained_tokenizers/SAMSum_BPE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0a22ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT_WINDOW = 100\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "D_MODEL = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "73f11a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(DF_DATA_PATH + 'train_df.json', orient = 'records', lines = True)\n",
    "val_df = pd.read_json(DF_DATA_PATH + 'val_df.json', orient = 'records', lines = True)\n",
    "test_df = pd.read_json(DF_DATA_PATH + 'test_df.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "319eede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The vocab size is 4000.\n",
      "The pad token index is 2.\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = train_and_save_tokenizer_for(in_file_paths = [BPE_IN_PATH], out_file_dir_path = BPE_OUT_PATH, vocab_size = 4_000)\n",
    "pretrained_bpe_tokenizer = load_tokenizer_from(dir_path = BPE_OUT_PATH, model_max_length = 10000)\n",
    "\n",
    "VOCAB_SIZE = pretrained_bpe_tokenizer.vocab_size\n",
    "PAD_TOKEN_IDX = pretrained_bpe_tokenizer.pad_token_id\n",
    "\n",
    "print(f'The vocab size is {VOCAB_SIZE}.')\n",
    "print(f'The pad token index is {PAD_TOKEN_IDX}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "fbc2c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = CustomEmbedding(VOCAB_SIZE, D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5747ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_prefix_space(texts: list[str], include_SOS: bool = False):\n",
    "    return [('<SOS>' if include_SOS else '') + ' ' + text.lstrip() for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8a000cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a max_context_window of 100...\n",
      "The number of training samples went from 14732 to 5561\n",
      "The number of validation samples went from 818 to 325\n",
      "The number of test samples went from 819 to 306\n"
     ]
    }
   ],
   "source": [
    "FILTER_tokenized_train_sources = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(train_df['dialogue'].tolist()),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_train_targets = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(train_df['summary'].tolist(), include_SOS = True),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_val_sources = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(val_df['dialogue'].tolist()),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_val_targets = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(val_df['summary'].tolist(), include_SOS = True),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_test_sources = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(test_df['dialogue'].tolist()),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "FILTER_tokenized_test_targets = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(test_df['summary'].tolist(), include_SOS = True),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "valid_src_train_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_train_sources.data['input_ids']])\n",
    "valid_src_val_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_val_sources.data['input_ids']])\n",
    "valid_src_test_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_test_sources.data['input_ids']])\n",
    "\n",
    "valid_tgt_train_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_train_targets.data['input_ids']])\n",
    "valid_tgt_val_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_val_targets.data['input_ids']])\n",
    "valid_tgt_test_indices = np.array([len(example) <= MAX_CONTEXT_WINDOW for example in FILTER_tokenized_test_targets.data['input_ids']])\n",
    "\n",
    "valid_train_df = train_df.iloc[valid_src_train_indices & valid_tgt_train_indices]\n",
    "valid_val_df = val_df.iloc[valid_src_val_indices & valid_tgt_val_indices]\n",
    "valid_test_df = test_df.iloc[valid_src_test_indices & valid_tgt_test_indices]\n",
    "\n",
    "print(f'With a max_context_window of {MAX_CONTEXT_WINDOW}...')\n",
    "print(f'The number of training samples went from {train_df.shape[0]} to {valid_train_df.shape[0]}')\n",
    "print(f'The number of validation samples went from {val_df.shape[0]} to {valid_val_df.shape[0]}')\n",
    "print(f'The number of test samples went from {test_df.shape[0]} to {valid_test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "686aeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_sources = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(valid_train_df['dialogue'].tolist()),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_train_targets = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(valid_train_df['summary'].tolist(), include_SOS = True),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_train_labels = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space((valid_train_df['summary'] + '<EOS>').tolist()),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_val_sources = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(valid_val_df['dialogue'].tolist()),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_val_targets = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(valid_val_df['summary'].tolist(), include_SOS = True),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_val_labels = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space((valid_val_df['summary'] + '<EOS>').tolist()),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_test_sources = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(valid_test_df['dialogue'].tolist()),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_test_targets = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space(valid_test_df['summary'].tolist(), include_SOS = True),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")\n",
    "\n",
    "tokenized_test_labels = pretrained_bpe_tokenizer(\n",
    "    normalize_prefix_space((valid_test_df['summary'] + '<EOS>').tolist()),\n",
    "    add_special_tokens = False,\n",
    "    return_attention_mask = False,\n",
    "    return_token_type_ids = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "28756f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextSummarizationDataset(tokenized_train_sources.data['input_ids'], tokenized_train_targets.data['input_ids'], tokenized_train_labels.data['input_ids'])\n",
    "val_ds = TextSummarizationDataset(tokenized_val_sources.data['input_ids'], tokenized_val_targets.data['input_ids'], tokenized_val_labels.data['input_ids'])\n",
    "test_ds = TextSummarizationDataset(tokenized_test_sources.data['input_ids'], tokenized_test_targets.data['input_ids'], tokenized_test_labels.data['input_ids'])\n",
    "\n",
    "# NOTE: Option to use HuggingFace DataCollatorWithPadding : requires changing TextSummarizationDataset __getitem__\n",
    "train_dataloader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))\n",
    "val_dataloader = DataLoader(val_ds, batch_size = BATCH_SIZE, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))\n",
    "test_dataloader = DataLoader(test_ds, batch_size = BATCH_SIZE, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f5508a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 367, 2504,   30,  ...,    2,    2,    2],\n",
      "        [ 604,  799,   69,  ...,    2,    2,    2],\n",
      "        [ 331, 2310,   30,  ...,    2,    2,    2],\n",
      "        ...,\n",
      "        [1306,   30,  674,  ...,    2,    2,    2],\n",
      "        [ 988,   30, 1124,  ...,    2,    2,    2],\n",
      "        [2763,   30,  680,  ...,    2,    2,    2]])\n",
      "tensor([[   0,  367, 2504,  ...,    2,    2,    2],\n",
      "        [   0,  331,  389,  ...,    2,    2,    2],\n",
      "        [   0,  331, 2310,  ...,  365,  996,   18],\n",
      "        ...,\n",
      "        [   0,  328,  281,  ...,    2,    2,    2],\n",
      "        [   0,  988,  317,  ...,    2,    2,    2],\n",
      "        [   0, 1798,  309,  ...,    2,    2,    2]])\n",
      "tensor([[ 367, 2504,   16,  ...,    2,    2,    2],\n",
      "        [ 331,  389,  382,  ...,    2,    2,    2],\n",
      "        [ 331, 2310,  509,  ...,  996,   18,    1],\n",
      "        ...,\n",
      "        [ 328,  281, 2046,  ...,    2,    2,    2],\n",
      "        [ 988,  317,   78,  ...,    2,    2,    2],\n",
      "        [1798,  309, 2763,  ...,    2,    2,    2]])\n"
     ]
    }
   ],
   "source": [
    "(source, target), label = next(iter(train_dataloader))\n",
    "print(source)\n",
    "print(target)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9399cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_IDX, reduction = 'sum')\n",
    "\n",
    "model = EncoderDecoderTransformer(\n",
    "    embeddings = embeddings,\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    d_model = D_MODEL,\n",
    "    num_attention_heads = 4,\n",
    "    num_encoder_layers = 1,\n",
    "    num_decoder_layers = 1,\n",
    "    dim_feedforward = 32,\n",
    "    dropout = 0.0,\n",
    "    max_context_window = MAX_CONTEXT_WINDOW,\n",
    "    use_pre_lnorm = True\n",
    ")\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr = 1e-4, momentum = 0.9, weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d0ddd08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:07<00:00, 11.88it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 30.84it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 13.15it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 30.43it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 13.18it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 23.39it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 12.83it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 24.89it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 12.91it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 30.74it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 13.28it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 32.12it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 13.23it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 31.23it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 13.29it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 29.69it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 12.54it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 26.48it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 13.10it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 30.98it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 13.51it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 32.48it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 13.06it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 30.91it/s]\n",
      "100%|██████████| 87/87 [00:07<00:00, 12.38it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 30.69it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 13.29it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 28.77it/s]\n",
      "100%|██████████| 87/87 [00:06<00:00, 12.66it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 31.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144.41541609535156, 129.68088295930926, 125.27613029075256, 121.95349360361222, 119.49773086632912, 117.46702762542708, 115.83128429646759, 114.47991408849015, 113.25448303826313, 112.21780504714755, 111.26309912574739, 110.4387042397669, 109.54869712057184, 108.8436748392825, 108.14483206847915]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.03770503471947612, 0.051263797729779625, 0.056441929635937, 0.06067534980997495, 0.06301493431003177, 0.06598223826093083, 0.06738570721883995, 0.06877961502919723, 0.06942644992347707, 0.0701620149654899, 0.07182874259089037, 0.07183013575410399, 0.07264144991087906, 0.07253662853070846, 0.07379351367108254]\n",
      "\n",
      "[134.98083364633413, 129.48837665264423, 125.80706486628605, 122.8987714092548, 121.08897536057692, 119.47872013972356, 118.6186923452524, 117.09996300330529, 116.32009446364182, 115.45841796875, 115.09649864783654, 114.21207688551682, 113.73935302734375, 113.24013953575721, 112.70538461538462]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.04618083839839589, 0.051444326085594336, 0.056707813772792784, 0.05877561250704932, 0.061783319756876995, 0.0622219437308102, 0.061031392944420074, 0.06491634814211417, 0.06428974246506673, 0.06560561438686635, 0.06685882574096122, 0.06773607368882763, 0.06867598220439877, 0.06754809198571339, 0.06880130333980826]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "training_losses = list()\n",
    "training_sequence_accuracies = list()\n",
    "training_token_accuracies = list()\n",
    "\n",
    "gold_validation_losses = list()\n",
    "gold_validation_sequence_accuracies = list()\n",
    "gold_validation_token_accuracies = list()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    print(f'Running epoch {i+1}...')\n",
    "\n",
    "    training_loss, training_sequence_accuracy, training_token_accuracy = run_train_epoch(train_dataloader, model, loss_fn, optim, calculate_sequence_accuracy = True, calculate_token_accuracy = True)\n",
    "\n",
    "    training_losses.append(training_loss)\n",
    "    training_sequence_accuracies.append(training_sequence_accuracy)\n",
    "    training_token_accuracies.append(training_token_accuracy)\n",
    "\n",
    "    gold_val_loss, gold_val_sequence_accuracy, gold_val_token_accuracy = run_gold_validation_loop(val_dataloader, model, loss_fn, calculate_sequence_accuracy = True, calculate_token_accuracy = True)\n",
    "    \n",
    "    gold_validation_losses.append(gold_val_loss)\n",
    "    gold_validation_sequence_accuracies.append(gold_val_sequence_accuracy)\n",
    "    gold_validation_token_accuracies.append(gold_val_token_accuracy)\n",
    "\n",
    "print(training_losses)\n",
    "print(training_sequence_accuracies)\n",
    "print(training_token_accuracies)\n",
    "\n",
    "print()\n",
    "\n",
    "print(gold_validation_losses)\n",
    "print(gold_validation_sequence_accuracies)\n",
    "print(gold_validation_token_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
