{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "from embedding import CustomEmbedding\n",
    "from transformers import EncoderDecoderTransformer\n",
    "from utils import padding_collate_fn\n",
    "\n",
    "from generate_data import RandomIntegerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4521,  0.7097,  1.0505, -0.6166, -1.0084,  1.5881, -0.4116,  0.0634,\n",
      "         -1.4550, -0.4137,  0.1941, -0.7222, -0.7129,  0.9572, -0.2055, -0.3407,\n",
      "          0.1691, -0.4828,  1.8598,  0.3019, -0.8803, -0.1361, -0.8615,  0.1109,\n",
      "         -0.4062, -0.6712,  0.2429, -1.2812, -0.1855, -0.2590, -2.9673,  0.6972],\n",
      "        [ 2.0117, -0.3468, -0.2348,  0.3069,  0.3879, -0.7460,  0.8612,  1.1340,\n",
      "         -0.4244, -2.9503,  1.7311, -1.0154,  1.6944, -0.3789,  0.9265, -0.7821,\n",
      "          0.8744,  0.1687,  1.2608, -1.3116,  0.5958,  0.5129,  0.4727,  0.2873,\n",
      "          0.6193,  2.2453,  0.2456,  1.1684,  0.8631, -0.5087,  0.4214,  2.7316]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_real_tokens = 10\n",
    "PAD_TOKEN_IDX = n_real_tokens\n",
    "SOS_TOKEN_IDX = n_real_tokens + 1\n",
    "EOS_TOKEN_IDX = n_real_tokens + 2\n",
    "vocab_size = n_real_tokens + 3\n",
    "D_MODEL = 32\n",
    "\n",
    "embeddings = CustomEmbedding(vocab_size, d_model = D_MODEL) # 3 = PAD, SOS, EOS\n",
    "\n",
    "indices = torch.tensor([1,9])\n",
    "\n",
    "# print(embeddings.embeddings.weight)\n",
    "print(embeddings.embeddings(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT_WINDOW = 50\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MIN_SEQ_LEN = 2\n",
    "MAX_SEQ_LEN = min(10, MAX_CONTEXT_WINDOW)\n",
    "\n",
    "NUM_TRAINING_SEQUENCES = 10000\n",
    "NUM_VALIDATION_SEQUENCES = 1000\n",
    "\n",
    "VOCAB = [i for i in range(n_real_tokens)] # non-sos,eos,pad\n",
    "\n",
    "train_rand_ds = RandomIntegerDataset(MIN_SEQ_LEN, MAX_SEQ_LEN, NUM_TRAINING_SEQUENCES, VOCAB)\n",
    "train_dataloader = DataLoader(train_rand_ds, batch_size = BATCH_SIZE, shuffle = True, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))\n",
    "\n",
    "val_rand_ds = RandomIntegerDataset(MIN_SEQ_LEN, MAX_SEQ_LEN, NUM_VALIDATION_SEQUENCES, VOCAB)\n",
    "val_dataloader = DataLoader(val_rand_ds, batch_size = BATCH_SIZE, collate_fn = partial(padding_collate_fn, pad_token_idx = PAD_TOKEN_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8,  2,  6,  9,  1,  8,  8,  1,  4,  2],\n",
      "        [ 8,  5,  8,  4,  0,  6,  7,  4, 10, 10],\n",
      "        [ 5,  9,  7, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 6,  6,  7,  4,  0,  1,  1,  6,  4, 10],\n",
      "        [ 2,  7,  5,  4,  6,  6, 10, 10, 10, 10],\n",
      "        [ 2,  5,  1,  5,  2,  9, 10, 10, 10, 10],\n",
      "        [ 5,  6,  5,  7,  8,  4,  0,  2,  0,  9],\n",
      "        [ 8,  4,  5,  4,  8,  2,  6,  9,  8, 10],\n",
      "        [ 5,  3,  0, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 5,  1,  6,  0,  6,  2,  7,  8,  6,  6],\n",
      "        [ 1,  1,  0,  8,  6, 10, 10, 10, 10, 10],\n",
      "        [ 2,  7,  1,  4,  0,  9,  8,  2, 10, 10],\n",
      "        [ 3,  9, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 9,  3,  2,  6,  7,  7,  3, 10, 10, 10],\n",
      "        [ 1,  1,  2,  2,  6, 10, 10, 10, 10, 10],\n",
      "        [ 9,  9,  1,  7,  7,  9,  8, 10, 10, 10],\n",
      "        [ 8,  2,  4,  2,  4, 10, 10, 10, 10, 10],\n",
      "        [ 8,  6, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 8,  1,  8,  8,  6,  7,  9,  2,  3, 10],\n",
      "        [ 5,  0,  8,  7,  1,  8, 10, 10, 10, 10],\n",
      "        [ 9,  1,  1,  3,  1,  9, 10, 10, 10, 10],\n",
      "        [ 3,  2,  3,  0,  5,  9,  2,  4,  9,  4],\n",
      "        [ 5,  3,  8,  9,  8, 10, 10, 10, 10, 10],\n",
      "        [ 3,  2,  5,  0,  6,  8,  5,  3,  4, 10],\n",
      "        [ 1,  4,  8,  6,  5,  6, 10, 10, 10, 10],\n",
      "        [ 0,  6,  9,  6,  5,  1,  3,  2,  1,  2],\n",
      "        [ 8,  6,  0,  9, 10, 10, 10, 10, 10, 10],\n",
      "        [ 9,  2, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 8,  8,  3,  5, 10, 10, 10, 10, 10, 10],\n",
      "        [ 4,  4,  7,  2,  8,  5, 10, 10, 10, 10],\n",
      "        [ 1,  3,  1,  1,  5,  5,  7,  6,  1,  5],\n",
      "        [ 4,  2,  0,  5,  9,  1,  2,  5, 10, 10]])\n",
      "tensor([[11,  1,  1,  2,  2,  4,  6,  8,  8,  8,  9],\n",
      "        [11,  0,  4,  4,  5,  6,  7,  8,  8, 10, 10],\n",
      "        [11,  5,  7,  9, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  1,  1,  4,  4,  6,  6,  6,  7, 10],\n",
      "        [11,  2,  4,  5,  6,  6,  7, 10, 10, 10, 10],\n",
      "        [11,  1,  2,  2,  5,  5,  9, 10, 10, 10, 10],\n",
      "        [11,  0,  0,  2,  4,  5,  5,  6,  7,  8,  9],\n",
      "        [11,  2,  4,  4,  5,  6,  8,  8,  8,  9, 10],\n",
      "        [11,  0,  3,  5, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  1,  2,  5,  6,  6,  6,  6,  7,  8],\n",
      "        [11,  0,  1,  1,  6,  8, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  1,  2,  2,  4,  7,  8,  9, 10, 10],\n",
      "        [11,  3,  9, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  2,  3,  3,  6,  7,  7,  9, 10, 10, 10],\n",
      "        [11,  1,  1,  2,  2,  6, 10, 10, 10, 10, 10],\n",
      "        [11,  1,  7,  7,  8,  9,  9,  9, 10, 10, 10],\n",
      "        [11,  2,  2,  4,  4,  8, 10, 10, 10, 10, 10],\n",
      "        [11,  6,  8, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  1,  2,  3,  6,  7,  8,  8,  8,  9, 10],\n",
      "        [11,  0,  1,  5,  7,  8,  8, 10, 10, 10, 10],\n",
      "        [11,  1,  1,  1,  3,  9,  9, 10, 10, 10, 10],\n",
      "        [11,  0,  2,  2,  3,  3,  4,  4,  5,  9,  9],\n",
      "        [11,  3,  5,  8,  8,  9, 10, 10, 10, 10, 10],\n",
      "        [11,  0,  2,  3,  3,  4,  5,  5,  6,  8, 10],\n",
      "        [11,  1,  4,  5,  6,  6,  8, 10, 10, 10, 10],\n",
      "        [11,  0,  1,  1,  2,  2,  3,  5,  6,  6,  9],\n",
      "        [11,  0,  6,  8,  9, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  2,  9, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  3,  5,  8,  8, 10, 10, 10, 10, 10, 10],\n",
      "        [11,  2,  4,  4,  5,  7,  8, 10, 10, 10, 10],\n",
      "        [11,  1,  1,  1,  1,  3,  5,  5,  5,  6,  7],\n",
      "        [11,  0,  1,  2,  2,  4,  5,  5,  9, 10, 10]])\n",
      "tensor([[ 1,  1,  2,  2,  4,  6,  8,  8,  8,  9, 12],\n",
      "        [ 0,  4,  4,  5,  6,  7,  8,  8, 12, 10, 10],\n",
      "        [ 5,  7,  9, 12, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  1,  1,  4,  4,  6,  6,  6,  7, 12, 10],\n",
      "        [ 2,  4,  5,  6,  6,  7, 12, 10, 10, 10, 10],\n",
      "        [ 1,  2,  2,  5,  5,  9, 12, 10, 10, 10, 10],\n",
      "        [ 0,  0,  2,  4,  5,  5,  6,  7,  8,  9, 12],\n",
      "        [ 2,  4,  4,  5,  6,  8,  8,  8,  9, 12, 10],\n",
      "        [ 0,  3,  5, 12, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 0,  1,  2,  5,  6,  6,  6,  6,  7,  8, 12],\n",
      "        [ 0,  1,  1,  6,  8, 12, 10, 10, 10, 10, 10],\n",
      "        [ 0,  1,  2,  2,  4,  7,  8,  9, 12, 10, 10],\n",
      "        [ 3,  9, 12, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 2,  3,  3,  6,  7,  7,  9, 12, 10, 10, 10],\n",
      "        [ 1,  1,  2,  2,  6, 12, 10, 10, 10, 10, 10],\n",
      "        [ 1,  7,  7,  8,  9,  9,  9, 12, 10, 10, 10],\n",
      "        [ 2,  2,  4,  4,  8, 12, 10, 10, 10, 10, 10],\n",
      "        [ 6,  8, 12, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 1,  2,  3,  6,  7,  8,  8,  8,  9, 12, 10],\n",
      "        [ 0,  1,  5,  7,  8,  8, 12, 10, 10, 10, 10],\n",
      "        [ 1,  1,  1,  3,  9,  9, 12, 10, 10, 10, 10],\n",
      "        [ 0,  2,  2,  3,  3,  4,  4,  5,  9,  9, 12],\n",
      "        [ 3,  5,  8,  8,  9, 12, 10, 10, 10, 10, 10],\n",
      "        [ 0,  2,  3,  3,  4,  5,  5,  6,  8, 12, 10],\n",
      "        [ 1,  4,  5,  6,  6,  8, 12, 10, 10, 10, 10],\n",
      "        [ 0,  1,  1,  2,  2,  3,  5,  6,  6,  9, 12],\n",
      "        [ 0,  6,  8,  9, 12, 10, 10, 10, 10, 10, 10],\n",
      "        [ 2,  9, 12, 10, 10, 10, 10, 10, 10, 10, 10],\n",
      "        [ 3,  5,  8,  8, 12, 10, 10, 10, 10, 10, 10],\n",
      "        [ 2,  4,  4,  5,  7,  8, 12, 10, 10, 10, 10],\n",
      "        [ 1,  1,  1,  1,  3,  5,  5,  5,  6,  7, 12],\n",
      "        [ 0,  1,  2,  2,  4,  5,  5,  9, 12, 10, 10]])\n"
     ]
    }
   ],
   "source": [
    "input, label = next(iter(train_dataloader))\n",
    "print(input[0])\n",
    "print(input[1])\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_IDX, reduction = 'sum')\n",
    "\n",
    "model = EncoderDecoderTransformer(\n",
    "                    embeddings = embeddings, \n",
    "                    vocab_size = vocab_size, \n",
    "                    d_model = D_MODEL, \n",
    "                    num_attention_heads = 4, \n",
    "                    num_encoder_layers = 1, \n",
    "                    num_decoder_layers = 1, \n",
    "                    dim_feedforward = 32, \n",
    "                    dropout = 0.0,\n",
    "                    max_context_window = MAX_CONTEXT_WINDOW,\n",
    "                    use_pre_lnorm = True)\n",
    "\n",
    "optim = torch.optim.SGD(params = model.parameters(), lr = 1e-3, momentum = 0.9, weight_decay = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(source: torch.Tensor, model: nn.Module) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    batch_size = source.size(dim = 0)\n",
    "\n",
    "    encoder_output, source_pad_mask = model.encode(source)\n",
    "\n",
    "    # target will contain num_batch sequences of indices that are the predicted next-words for each batch element\n",
    "    target = torch.full((batch_size, 1), SOS_TOKEN_IDX)\n",
    "\n",
    "    while (target[:, -1] == PAD_TOKEN_IDX).sum() < batch_size and target.size(dim = 1) <= MAX_CONTEXT_WINDOW:\n",
    "\n",
    "        decoder_output, _ = model.decode(target, encoder_output, source_pad_mask)\n",
    "        pred_logits = model.project_into_vocab(decoder_output)\n",
    "\n",
    "        # pred_logits.shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        print('pred_logits.shape:', pred_logits.shape)\n",
    "\n",
    "        last_row_pred_logits = pred_logits[:, -1, :]\n",
    "\n",
    "        # last_row_pred_logits.shape == [batch_size, 1, vocab_size]\n",
    "        print('last_row_pred_logits.shape:', last_row_pred_logits.shape)\n",
    "\n",
    "        predictions = torch.argmax(last_row_pred_logits, dim = -1)\n",
    "\n",
    "        # predictions.shape: [batch_size, 1]\n",
    "\n",
    "        # print(target)\n",
    "        # print(predictions)\n",
    "        print('target.shape:', target.shape)\n",
    "        print('predictions.shape:', predictions.shape)\n",
    "\n",
    "        target = torch.concat((target, predictions.reshape(-1, 1)), dim = 1)\n",
    "\n",
    "        print()\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_epoch(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, optimizer: torch.optim.Optimizer, calculate_sequence_accuracy: bool = False, calculate_token_accuracy: bool = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    num_sequences = len(dataloader.dataset)\n",
    "    num_tokens = 0\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    total_correct_sequences = 0\n",
    "    total_correct_tokens = 0\n",
    "\n",
    "    for (source, target), label in tqdm(dataloader):\n",
    "\n",
    "        # FORWARD\n",
    "        pred_logits = model(source, target)\n",
    "\n",
    "        # pred_logits.shape: [batch_size, seq_len, vocab_size]\n",
    "        # label.shape: [batch_size, seq_len]\n",
    "\n",
    "        # CrossEntropyLoss (loss_fn) only takes 2D predictions (n_batch * seq_len, vocab_size) and 1D labels (n_batch * seq_len)\n",
    "        batch_loss = loss_fn(pred_logits.view(-1, pred_logits.size(-1)), label.view(-1))\n",
    "\n",
    "        # LOG\n",
    "        with torch.no_grad():\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "            predictions = torch.argmax(pred_logits, dim = -1) # predictions.shape: [batch_size, seq_len]\n",
    "            match_matrix = torch.eq(predictions, label)\n",
    "\n",
    "            if calculate_sequence_accuracy:\n",
    "                num_correct_sequences = torch.all(match_matrix, dim = 1).sum()\n",
    "                total_correct_sequences += num_correct_sequences.item()\n",
    "\n",
    "            if calculate_token_accuracy:\n",
    "                num_correct_tokens = match_matrix.sum()      \n",
    "                total_correct_tokens += num_correct_tokens.item()\n",
    "\n",
    "                num_tokens += torch.numel(label)\n",
    "\n",
    "        # BACKWARD\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # OPTIMIZE\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    average_epoch_loss = epoch_loss / num_sequences\n",
    "    average_epoch_sequence_accuracy = total_correct_sequences / num_sequences if calculate_sequence_accuracy else None\n",
    "    average_epoch_token_accuracy = total_correct_tokens / num_tokens if calculate_token_accuracy else None\n",
    "\n",
    "    return average_epoch_loss, average_epoch_sequence_accuracy, average_epoch_token_accuracy\n",
    "\n",
    "def run_gold_validation_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, calculate_sequence_accuracy: bool = False, calculate_token_accuracy: bool = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    num_sequences = len(dataloader.dataset)\n",
    "    num_tokens = 0\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    total_correct_sequences = 0\n",
    "    total_correct_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (source, target), label in tqdm(dataloader):\n",
    "            \n",
    "            # FORWARD\n",
    "            pred_logits = model(source, target)\n",
    "            batch_loss = loss_fn(pred_logits.view(-1, pred_logits.size(-1)), label.view(-1))\n",
    "\n",
    "            # LOG\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "            predictions = torch.argmax(pred_logits, dim = -1) # predictions.shape: [batch_size, seq_len]\n",
    "            match_matrix = torch.eq(predictions, label)\n",
    "\n",
    "            if calculate_sequence_accuracy:\n",
    "                num_correct_sequences = torch.all(match_matrix, dim = 1).sum()\n",
    "                total_correct_sequences += num_correct_sequences.item()\n",
    "\n",
    "            if calculate_token_accuracy:\n",
    "                num_correct_tokens = match_matrix.sum()      \n",
    "                total_correct_tokens += num_correct_tokens.item()\n",
    "\n",
    "                num_tokens += torch.numel(label)\n",
    "\n",
    "    average_epoch_loss = epoch_loss / num_sequences\n",
    "    average_epoch_sequence_accuracy = total_correct_sequences / num_sequences if calculate_sequence_accuracy else None\n",
    "    average_epoch_token_accuracy = total_correct_tokens / num_tokens if calculate_token_accuracy else None\n",
    "\n",
    "    return average_epoch_loss, average_epoch_sequence_accuracy, average_epoch_token_accuracy\n",
    "\n",
    "def run_autoregressive_validation_loop(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (source, _), label in tqdm(dataloader):\n",
    "\n",
    "            # FORWARD\n",
    "            pred_indices = greedy_decode(source, model)\n",
    "            print(pred_indices)\n",
    "            # batch_loss = loss_fn(..., ...)\n",
    "\n",
    "            # LOG\n",
    "            # epoch_loss += batch_loss.item()\n",
    "\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_logits.shape: torch.Size([32, 1, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 1])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 2, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 2])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 3, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 3])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 4, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 4])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 5, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 5])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 6, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 6])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 7, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 7])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 8, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 8])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 9, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 9])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 10, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 10])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 11, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 11])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 12, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 12])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 13, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 13])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 14, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 14])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 15, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 15])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 16, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 16])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 17, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 17])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 18, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 18])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 19, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 19])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 20, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 20])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 21, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 21])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 22, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 22])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 23, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 23])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 24, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 24])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 25, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 25])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 26, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 26])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 27, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 27])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 28, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 28])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 29, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 29])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 30, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 30])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 31, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 31])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 32, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 32])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 33, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 33])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 34, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 34])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 35, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 35])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 36, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 36])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 37, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 37])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 38, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 38])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 39, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 39])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 40, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 40])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 41, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 41])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 42, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 42])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 43, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 43])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 44, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 44])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 45, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 45])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 46, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 46])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 47, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 47])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 48, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 48])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 49, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 49])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "pred_logits.shape: torch.Size([32, 50, 13])\n",
      "last_row_pred_logits.shape: torch.Size([32, 13])\n",
      "target.shape: torch.Size([32, 50])\n",
      "predictions.shape: torch.Size([32])\n",
      "\n",
      "tensor([[11,  9,  3,  ...,  8,  8,  8],\n",
      "        [11,  9,  3,  ...,  8,  8,  8],\n",
      "        [11,  9,  3,  ...,  8,  8,  8],\n",
      "        ...,\n",
      "        [11,  9,  3,  ...,  8,  8,  8],\n",
      "        [11,  9,  3,  ...,  8,  8,  8],\n",
      "        [11,  9,  1,  ...,  8,  8,  8]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_autoregressive_validation_loop(val_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 113.69it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 312.40it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 116.41it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 304.74it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 117.62it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 303.17it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 118.31it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 315.31it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 118.65it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 321.20it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 123.55it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 317.47it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 123.65it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 316.87it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 117.97it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 292.90it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 117.72it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 309.24it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 121.90it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 309.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.433036536407471, 1.772785053062439, 1.782198503112793, 1.4157891374588012, 1.3334199272155762, 1.1036983204126358, 1.186024510860443, 0.9799411701440811, 1.0434875957012177, 0.8846561305046081]\n",
      "[0.008, 0.0272, 0.0326, 0.0364, 0.0439, 0.0495, 0.0478, 0.0531, 0.0524, 0.0603]\n",
      "[0.514817241881462, 0.5753443375601224, 0.5778949861536219, 0.5872469783020242, 0.5911029304563348, 0.5977832871518156, 0.5958861681970559, 0.6020326387877022, 0.5998452294246176, 0.6056445739257101]\n",
      "\n",
      "[2.105001425743103, 1.9623536720275878, 4.119647649765015, 1.3570107583999633, 1.1826796026229858, 0.7914193334579468, 0.962474100112915, 1.753965274810791, 0.7659035215377807, 0.7691247215270997]\n",
      "[0.008, 0.015, 0.001, 0.042, 0.067, 0.04, 0.051, 0.04, 0.064, 0.061]\n",
      "[0.5802334062727936, 0.587709700948213, 0.5289934354485777, 0.598741794310722, 0.6091356673960613, 0.6198942377826404, 0.6146973012399708, 0.5888037928519329, 0.6227206418672502, 0.6208059810357404]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "training_losses = list()\n",
    "training_sequence_accuracies = list()\n",
    "training_token_accuracies = list()\n",
    "\n",
    "gold_validation_losses = list()\n",
    "gold_validation_sequence_accuracies = list()\n",
    "gold_validation_token_accuracies = list()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # print(f'Running epoch {i+1}...')\n",
    "\n",
    "    training_loss, training_sequence_accuracy, training_token_accuracy = run_train_epoch(train_dataloader, model, loss_fn, optim, calculate_sequence_accuracy = True, calculate_token_accuracy = True)\n",
    "\n",
    "    training_losses.append(training_loss)\n",
    "    training_sequence_accuracies.append(training_sequence_accuracy)\n",
    "    training_token_accuracies.append(training_token_accuracy)\n",
    "\n",
    "    gold_val_loss, gold_val_sequence_accuracy, gold_val_token_accuracy = run_gold_validation_loop(val_dataloader, model, loss_fn, calculate_sequence_accuracy = True, calculate_token_accuracy = True)\n",
    "    gold_validation_losses.append(gold_val_loss)\n",
    "    gold_validation_sequence_accuracies.append(gold_val_sequence_accuracy)\n",
    "    gold_validation_token_accuracies.append(gold_val_token_accuracy)\n",
    "\n",
    "print(training_losses)\n",
    "print(training_sequence_accuracies)\n",
    "print(training_token_accuracies)\n",
    "\n",
    "print()\n",
    "\n",
    "print(gold_validation_losses)\n",
    "print(gold_validation_sequence_accuracies)\n",
    "print(gold_validation_token_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
